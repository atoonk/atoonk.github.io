<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Andree's Musings]]></title><description><![CDATA[Thoughts, stories and wild ideas from the world of Infrastructure Engineering.]]></description><link>http://toonk.io/</link><image><url>http://toonk.io/favicon.png</url><title>Andree&apos;s Musings</title><link>http://toonk.io/</link></image><generator>Ghost 3.34</generator><lastBuildDate>Sun, 17 Sep 2023 21:37:40 GMT</lastBuildDate><atom:link href="http://toonk.io/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[AWS IPv4 Estate Now Worth $4.5 Billion]]></title><description><![CDATA[AWS grew its IPv4 estate with an additional 27 million IP addresses to now owning 128 Million IPv4 addresses. At a value of $35 per IPv4 address, the total value of AWS’ IPv4 estate is ~4.5 Billion dollars. An increase of $2 billion]]></description><link>http://toonk.io/aws-ipv4-estate-now-worth-4-5-billion/</link><guid isPermaLink="false">6507705ef554aea7ffa44089</guid><category><![CDATA[ipv4]]></category><category><![CDATA[aws]]></category><category><![CDATA[networking]]></category><dc:creator><![CDATA[Andree Toonk]]></dc:creator><pubDate>Sun, 17 Sep 2023 21:36:54 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1613297595539-0d36a37702c6?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDJ8fEJyaXRpc2glMjBDb2x1bWJpYXxlbnwwfHx8fDE2OTQ5ODYzNzJ8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1613297595539-0d36a37702c6?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDJ8fEJyaXRpc2glMjBDb2x1bWJpYXxlbnwwfHx8fDE2OTQ5ODYzNzJ8MA&ixlib=rb-4.0.3&q=80&w=2000" alt="AWS IPv4 Estate Now Worth $4.5 Billion"><p>Three years ago, I wrote a blog titled “<a href="https://toonk.io/aws-and-their-billions-in-ipv4-addresses/index.html" rel="noopener ugc nofollow">AWS and their Billions in IPv4 addresses</a> “in which I estimated AWS owned about $2.5 billion worth of IPv4 addresses. AWS has continued to grow incredibly, and so has its IPv4 usage. In fact, it’s grown so much that it will soon start to charge customers for IPv4 addresses! Enough reason to check in again, three years later, to see what AWS’ IPv4 estate looks like today.</p><h1 id="a-quick-2020-recap">A quick 2020 recap</h1><p>Let’s first quickly summarize what we learned when <a href="https://toonk.io/aws-and-their-billions-in-ipv4-addresses/index.html" rel="noopener ugc nofollow">looking at AWS’s IPv4 usage in 2020</a>. First, in 2020, we observed that the total number of IPv4 addresses we could attribute to AWS was just over 100 Million (100,750,168). That’s the equivalent of just over six /8 blocks.</p><p>Second, for fun, we tried to put a number on it; back then, I used $25 per IP, bringing the estimated value of their IPv4 estate to Just over $2.5 billion.</p><p>Third, AWS publishes their actively used <a href="https://ip-ranges.amazonaws.com/ip-ranges.json" rel="noopener ugc nofollow">IPv4 addresses in a JSON</a> file. The JSON file contained references to roughly 53 Million IPv4 addresses. That meant they still had ~47 Million IPv4 addresses, or 47%, available for future allocations. That’s pretty healthy!</p><h1 id="the-2023-numbers">The 2023 numbers</h1><p>Okay, let’s look at the current data. Now, three years later, what does the IPv4 estate for AWS look like? I used the same scripts and methods as three years ago and found the following.</p><p>First, we observe that AWS currently owns 127,972,688 IPv4 addresses. Ie. almost 128 million IPv4 addresses. That’s an increase of 27 million IPv4 addresses. In other words, AWS added the equivalent of 1.6 /8’s or 415 /16’s in three years!</p><p>Second, what’s it worth? This is always tricky and just for fun. Let’s first assume the same $25 per IPv4 address we used in 2020.</p><p><em><em>127,972,688 ipv4 addresses x $25 per IP = $3,199,317,200.</em></em></p><blockquote><em><em><strong><strong>So, with the increase of IPv4 addresses, the value went up to ~$3.2 Billion. That’s a $700 million increase since 2020.</strong></strong></em></em></blockquote><p>However, if we consider the increase in IPv4 prices over the last few years, this number will be higher. Below is the total value of 127M IPv4 addresses at different market prices.</p><p>Total number of IPv4 addresses: 127,972,688<br>value at $20 per IP: $2,559,453,760<br>value at $25 per IP: $3,199,317,200<br>value at $30 per IP: $3,839,180,640<br><strong><strong>value at $35 per IP: $4,479,044,080</strong></strong><br>value at $40 per IP: $5,118,907,520<br>value at $50 per IP: $6,398,634,400</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://miro.medium.com/v2/resize:fit:1400/1*AgTw_knXi3Bz9CZTJ-bU_A.png" class="kg-image" alt="AWS IPv4 Estate Now Worth $4.5 Billion"><figcaption>IPv4 prices over time — Source: ipv4.global</figcaption></figure><p>Based on<a href="https://auctions.ipv4.global/" rel="noopener ugc nofollow"> this data from ipv4.global</a>, the average price for an IPv4 address is currently ~$35 dollars. With that estimate, we can determine <strong><strong>the value of AWS’s IPv4 estate today to about 4.5 Billion dollars. An increase of 2 Billion compared to three years ago!</strong></strong></p><p>Thirdly, let’s compare the difference between the IPv4 data we found and what’s published in the JSON file AWS makes available. In the JSON today, we count about 73 million IPv4 addresses (72,817,397); three years ago, that was 53 Million. So, an increase of 20 million in IPv4 addresses allocated to AWS services.</p><p>Finally, when we compare the ratio between what Amazon owns and what is allocated to AWS according to the JSON data, we observe that about 57% (72817397 / 127972688) of the IPv4 addresses have been (publicly) allocated to AWS service. They may still have 43% available for future use. That’s almost the same as three years ago when it was 47%.<br><em><em>(Note: this is an outsider’s perspective; we should likely assume not everything is used for AWS).</em></em></p><h1 id="where-did-the-growth-come-from">Where did the growth come from</h1><p>A quick comparison between the results from three years ago and now shows the following significant new additions to AWS’ IPv4 estate,</p><ul><li>Two new /11 allocations: 13.32.0.0/11 and 13.192.0.0/11. This whole 13/8 block was formerly owned by Xerox.<br>(<em><em>Note: it appears AWS owned 13.32.0.0/12 already in 2020)</em></em>.</li><li>Two new /12 allocations: 13.224.0.0/12 (see above as well). It appears they continued purchasing from that 13/8 block.</li><li>I’m also seeing more consolidation in the 16.0.0.0/8 block. AWS used to have quite a few /16 allocations from that block, which are now consolidated into three /12 allocations: 16.16.0.0/12 16.48.0.0/12, and 16.112.0.0/12</li><li>Finally, the 63.176.0.0/12 allocation is new.</li></ul><h1 id="aws-is-starting-to-charge-for-ipv4-addresses">AWS is starting to charge for IPv4 addresses</h1><p>In August of this year, <a href="https://aws.amazon.com/blogs/aws/new-aws-public-ipv4-address-charge-public-ip-insights/" rel="noopener ugc nofollow">AWS announced</a> they will start charging their customers for IPv4 addresses as of 2024.</p><p><em><em>Effective February 1, 2024 there will be a charge of $0.005 per IP per hour for all public IPv4 addresses, whether attached to a service or not (there is already a charge for public IPv4 addresses you allocate in your account but don’t attach to an EC2 instance).</em></em></p><p>That’s a total of $43.80 per year per IPv4 address; that’s a pretty hefty number! The reason for this is outlined in the same AWS blog:</p><p><em><em>As you may know, IPv4 addresses are an increasingly scarce resource and the cost to acquire a single public IPv4 address has risen more than 300% over the past 5 years. This change reflects our own costs and is also intended to encourage you to be a bit more frugal with your use of public IPv4 addresses</em></em></p><p>The 300% cost increase to acquire an IPv4 address is interesting and is somewhat reflected in our valuation calculation above (though we used a more conservative number).</p><p>So, how much money will AWS make from this new IPv4 charge? The significant variable here is how many IP addresses are used at any given time by AWS customers. Let’s explore a few scenarios, starting with a very conservative estimate, say 10% of what is published in their IPv4 JSON is in use for a year. That’s 7.3 Million IPv4 addresses x $43.80, almost $320 Million a year. At 25% usage, that’s nearly $800 Million a year. And at 31% usage, that’s a billion dollars!</p><blockquote><em><em>Notice that I’m using a fairly conservative number here, so it’s not unlikely for <strong><strong>AWS to make between $500 Million to a Billion dollars a year with this new charge!</strong></strong></em></em></blockquote><h1 id="the-data">The data</h1><p>You can find the data I used for this analysis on the link below. There, you’ll also find all the IPv4 prefixes and a brief summary. <a href="https://gist.github.com/atoonk/d8bded9d1137b26b3c615ab614222afd" rel="noopener ugc nofollow">https://gist.github.com/atoonk/d8bded9d1137b26b3c615ab614222afd</a><br>Similar data from 2020 can be <a href="https://gist.github.com/atoonk/b749305012ae5b86bacba9b01160df9f#all-prefixes" rel="noopener ugc nofollow">found here</a>.<br><em><em>PS. Let me know if someone knows the LACNIC or AFRINIC AWS resources, as those are not included in this data set.</em></em></p><h1 id="wrap-up">Wrap up</h1><p>In this article, we saw how, over the last three years, AWS grew its IPv4 estate with an additional 27 million IP addresses to now owning 128 Million IPv4 addresses. <strong><strong>At a value of $35 per IPv4 address, the total value of AWS’ IPv4 estate is ~4.5 Billion dollars. An increase of $2 billion </strong></strong>compared to what we looked at three years ago!</p><p>Regarding IPv4 capacity planning, it seems like the unallocated IPv4 address pool (defined as not being in the AWS JSON) has remained stable, and quite a bit of IPv4 addresses are available for future use.</p><p>All this buying of IPv4 addresses is expensive, and in response to the increase in IPv4 prices, AWS will soon start to charge its customers for IPv4 usage. <strong><strong>Based on my estimates, It’s not unlikely that AWS will generate between $500 million and $1 billion in additional revenue with this new charge. Long live IPv4!</strong></strong></p><p>Cheers<br>Andree</p>]]></content:encoded></item><item><title><![CDATA[Diving into AI: An Exploration of Embeddings and Vector Databases]]></title><description><![CDATA[Join me on my journey into AI and learning about embeddings, vectors and vector databases]]></description><link>http://toonk.io/diving-into-ai-an-exploration-of-embeddings-and-vector-databases/</link><guid isPermaLink="false">644f3afaa1d93a29d0c07f1c</guid><category><![CDATA[ai]]></category><dc:creator><![CDATA[Andree Toonk]]></dc:creator><pubDate>Mon, 01 May 2023 04:09:12 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1531610373189-61a2afd654e4?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE2fHx2YW5jb3V2ZXIlMjBhaXxlbnwwfHx8fDE2ODI5MTQwNTE&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1531610373189-61a2afd654e4?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE2fHx2YW5jb3V2ZXIlMjBhaXxlbnwwfHx8fDE2ODI5MTQwNTE&ixlib=rb-4.0.3&q=80&w=2000" alt="Diving into AI: An Exploration of Embeddings and Vector Databases"><p>With the help of ChatGTP, AI has officially changed everything we do. It’s only been a few months since chatGPT was released, and like many, I’ve been exploring how to best use it. I used it as a coding buddy, to brainstorm, to help with writing, etc.</p><p>The potential of this new technology blows me away. But as a technology enthusiast, I’d love to understand better how it works, or at least better understand some of the common terms and underlying technology. I keep hearing about Large Language Models (LLMs), vector databases, training, models, etc. I’d love to learn more about it, and what better way to just dive in, get my hands dirty and experiment with it?</p><p>So in today’s blog, I’m sharing some learnings on one of these building blocks, called embeddings. Why embeddings? Well, originally, I planned to learn more about Vector databases, but I quickly learned that in order to understand these better, I should start with vectors and embeddings.</p><h3 id="what-is-an-embedding">What is an embedding</h3><p>This is the definition from the <a href="https://platform.openai.com/docs/guides/embeddings" rel="noopener">OpenAI website</a></p><blockquote><em>An embedding is a vector (list) of floating point numbers. The </em><a href="https://platform.openai.com/docs/guides/embeddings/which-distance-function-should-i-use" rel="noopener"><em>distance</em></a><em> between two vectors measures their relatedness. Small distances suggest high relatedness, and large distances suggest low relatedness.</em></blockquote><p>Hhm, ok, but what does that really mean? Imagine you have a word, say hamburger. In order to use this in an LLM (large language model) like GPT, the LLM needs to know what it means. To do that, we can turn the word hamburger into an embedding. An embedding is essentially a (set of) numerical representations of a word, indicating its meaning. We call this the Vector.</p><p>With embeddings, we can now represent the words as vectors:</p><ul><li>dog: [0.2, -0.1, 0.5, …]</li><li>cat: [0.1, -0.3, 0.4, …]</li><li>fish: [-0.3, 0.6, -0.1, …]</li></ul><p>Notice that it’s a representation of the meaning (semantics) of the word. For example, the word embeddings for “dog” and “puppy” would be close together in the vector space because they share a similar meaning and often appear in similar contexts. In contrast, the embeddings for “dog” and “car” would be farther apart because their meanings and contexts are quite different.</p><p>It is this “Word embeddings” technology that enables semantic search, which goes beyond simple keyword matching to understand the meaning and context behind a query. “Semantic” refers to the similarity in meaning between words or phrases.</p><p>For example, traditional string matching would fail to connect the “<em>searching for something to eat</em>” query with the sentence “<em>the mouse is looking for food.</em>” However, with semantic search powered by word embeddings, a search engine recognizes that both phrases share a similar meaning, and it would successfully find the sentence.</p><p>Ok, great. Now that we somewhat understand how this works, how does it really work?</p><h3 id="turning-words-or-sentences-into-embeddings">Turning words or sentences into embeddings</h3><p>A word or sentence can be turned into an embedding (a vector representation) using the OpenAI API. To get an embedding, send your text string to the <a href="https://platform.openai.com/docs/api-reference/embeddings" rel="noopener">embeddings API endpoint</a> along with a choice of embedding model ID (e.g., text-embedding-ada-002). The response will contain an embedding you can extract, save, and use.</p><p>In my case, I’m using the Python API. Using this API, you can simply use the code below to turn the word hamburger into an embedding.</p><pre><code class="language-python">from openai.embeddings_utils import get_embedding
hamburger_embedding = get_embedding("hamburger", engine='text-embedding-ada-002')

# will look like something like [-0.01317964494228363, -0.001876765862107277, …</code></pre><p>If you have a text document, you would turn all the words or sentences from that document into embeddings. Once you’ve done that, you essentially have a semantic representation of the document as a series of vectors. These vectors capture the meaning and context of the individual words or sentences.</p><h3 id="finding-similarities">Finding Similarities</h3><p>Once you have embeddings for words or sentences, you can use them to find semantic similarities. A common approach to measuring the similarity between two embeddings is by calculating how close the vectors are to each other.</p><p>Calculating the distance between vectors is done by calculating the cosine similarity; if you’re really interested, you can read about that here. <a href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener">https://en.wikipedia.org/wiki/Cosine_similarity</a></p><p>Luckily the Python module that OpenAI ships has an implementation of this c<em>osine_similarity,</em> and you can simply use it like this:</p><pre><code class="language-python">import openai
from openai.embeddings_utils import get_embedding, cosine_similarity
openai.api_key = "&lt;YOUR OPENAI API KEY HERE&gt;"
embedding1 = get_embedding("the kids are in the house",engine='text-embedding-ada-002')
embedding2 = get_embedding("the children are home",engine="text-embedding-ada-002")
cosine_similarity(embedding1, embedding2)</code></pre><p>Which prints: <em>0.9387390865828703</em>, meaning they’re very close.</p><h3 id="a-real-life-example">A real-life example</h3><p>Below is a slightly longer example. It reads the document called words.csv, which looks like this:</p><pre><code class="language-csv">text
"red"
"potatoes"
"soda"
"cheese"
"water"
"blue"
"crispy"
"hamburger"
"coffee"
"green"
"milk"
"la croix"
"yellow"
"chocolate"
"french fries"
"latte"
"cake"
"brown"
"cheeseburger"
"espresso"
"cheesecake"
"black"
"mocha"
"fizzy"
"carbon"
"banana"
"sunshine"
"orange carrot"
"sun"
"hay"
"cookies"
"fish"</code></pre><p>The script below then calculates the embeddings for all these words and adds it all to a Panda data frame. Next, it will take a search term (hotdog) and calculates what words are closest to the word hotdog.<br></p><pre><code class="language-python">import openai
import pandas as pd
import numpy as np
from openai.embeddings_utils import get_embedding, cosine_similarity

openai.api_key = "&lt;YOUR OPENAI API KEY HERE&gt;"

# read the data
df = pd.read_csv('words.csv')

# Lamda to add embedding column
df['embedding'] = df['text'].apply(lambda x: get_embedding(x, engine='text-embedding-ada-002'))
# Safe it to a csv file, for caching later. So we dont need to call the API all the time
# You'd store this in a vector database
df.to_csv('word_embeddings.csv')
df = pd.read_csv('word_embeddings.csv')

# Convert the string representation of the embedding to a numpy array
# neeeded since we wrote it to a csv file
df['embedding'] = df['embedding'].apply(eval).apply(np.array)

# Hotdog is not in the CSV. Let calculate the embedding for it
search_term = "hotdog"
search_term_vector = get_embedding(search_term, engine="text-embedding-ada-002")

# now we can calculate the similarity between the search term and all the words in the CSV 
df["similarities"] = df['embedding'].apply(lambda x: cosine_similarity(x, search_term_vector))
# print the top 5 most similar words
print(df.sort_values("similarities", ascending=False).head(5))</code></pre><p></p><p>The code above prints this:</p><pre><code>Unnamed: 0          text                                          embedding  similarities
7            7     hamburger  [-0.01317964494228363, -0.001876765862107277, ...      0.913613
18          18  cheeseburger  [-0.01824556663632393, 0.00504859397187829, 0....      0.886365
14          14  french fries  [0.0014257068978622556, -0.016548126935958862,...      0.853839
3            3        cheese  [-0.0032112577464431524, -0.0088559715077281, ...      0.838909
13          13     chocolate  [0.0015315973432734609, -0.012976923026144505,...      0.830742</code></pre><p>Pretty neat, right?! It calculated that a hotdog is most similar to a hamburger, cheeseburger, and fries!</p><p>Let’s do one more thing! In the example below, we add the embeddings for milk and coffee together, just like a simple math addition.</p><p>We then again calculate what this new embedding is most similar to (hint, what do you call a drink that adds coffee to milk?).</p><pre><code class="language-python"># Let's make a copy of the data frame we created earlier, so we can compare the embeddings of two words
food_df = df.copy()
milk_vector = food_df['embedding'][10]
coffee_vector = food_df['embedding'][8]

# lets add the two vectors together
milk_coffee_vector = milk_vector + coffee_vector

# now calculate the similarity between the combined vector and all the words in the CSV
food_df["similarities"] = food_df['embedding'].apply(lambda x: cosine_similarity(x, milk_coffee_vector))

print(food_df.sort_values("similarities", ascending=False).head(5))</code></pre><p>The result is this</p><pre><code>Unnamed: 0 text embedding similarities
8 8 coffee [-0.0007212135824374855, -0.01943901740014553,… 0.959562
10 10 milk [0.0009238893981091678, -0.019352708011865616,… 0.959562
15 15 latte [-0.015634406358003616, -0.003936054650694132,… 0.905960
19 19 espresso [-0.02250547707080841, -0.012807613238692284, … 0.898178
22 22 mocha [-0.012473775073885918, -0.026152553036808968,… 0.889710</code></pre><p>Ha! Yes, it’s obviously similar to coffee and milk, as that’s what we started with, but next up, we see a latte! That’s pretty cool, right? Coffee + Milk = Late 😀</p><h3 id="vector-database">Vector database</h3><p>Now that we’ve seen how embeddings work and how they can be used to find semantic similarities, let’s talk about vector databases. In our example, we saw that calculating the embeddings was done using an API call to the OpenAI API. This can be slow and will cost you credits. That’s why, in the example code, we saved the calculated embeddings to a CSV file for caching purposes.</p><p>While this approach works for small-scale experiments, it may not be practical for large amounts of data or production environments where performance and scalability are important. This is where vector databases come in.</p><p>There are a few popular ones; a well-known one is Pinecone, but even <a href="https://innerjoin.bit.io/vector-similarity-search-in-postgres-with-bit-io-and-pgvector-c58ac34f408b" rel="noopener">Postgres</a> can be used as a vector database. These vector databases are specifically designed for storing, managing, and efficiently searching through large amounts of embeddings. They are optimized for high-dimensional vector data and can handle operations such as nearest neighbor search, which is crucial for finding the most similar items to a given query.</p><h3 id="wrap-up">Wrap up</h3><p>In this exploration of the technology behind LLMs and AI, I delved into some of the foundational building blocks that power these advanced systems; specifically, we looked at embeddings and vectors. My initial curiosity about vector databases and their potential applications for my own data led me to first understand the underlying principles and the importance of vectors. It’s pretty cool to see how easy it was to get going, thanks to the existing API’s and libraries.</p><p>Perhaps in another weekend adventure, I’ll look further into the next logical topic: vector databases. I’d also love to explore Langchain, a fascinating framework for developing applications powered by language models.</p><p>That’s it for now; thanks for reading!</p><p>Cheers<br> Andree</p>]]></content:encoded></item><item><title><![CDATA[IPv4 for sale - WIDE and APNIC selling 43.0.0.0/8]]></title><description><![CDATA[In this article, we'll look at the recent sale of the IPv4 range 43.0.0.0/8 by The Asia Pacific Internet Development Trust (APIDT). We'll look at who bought what and for how much?]]></description><link>http://toonk.io/ipv4-sale-wide-and-apnic-selling-43-8/</link><guid isPermaLink="false">616649d716a05e598c2a5cd1</guid><category><![CDATA[ipv4]]></category><category><![CDATA[aws]]></category><dc:creator><![CDATA[Andree Toonk]]></dc:creator><pubDate>Wed, 13 Oct 2021 03:03:18 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1557409518-691ebcd96038?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEzfHx0b2t5b3xlbnwwfHx8fDE2MzQwOTM3Mzc&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: html--><div style="background-color: #68C2F9; padding-top: 5px;
  padding-right: 20px;
  padding-bottom: 5px;
  padding-left: 20px;
            border-radius: 25px;">
    <sup>&#9432;</sup> 
       <strong>May 23 2022, article updates.</strong> <br> 
    I've made two updates to this blog. <br>
    The <a href="#update-heading1">first update:</a> adds information about the prefixes that hadn't been sold yet when this article was first published. As well as the move of some of the prefixes from AWS to the Chinese operators of AWS (SINET and NWCD)<br>
    The <a href="#update-heading2">Second update:</a> describes the exact amount APIDT made with the sale of this address space. APIDT recently filed its annual report, which outlines the exact amount. 
</div><!--kg-card-end: html--><img src="https://images.unsplash.com/photo-1557409518-691ebcd96038?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEzfHx0b2t5b3xlbnwwfHx8fDE2MzQwOTM3Mzc&ixlib=rb-1.2.1&q=80&w=2000" alt="IPv4 for sale - WIDE and APNIC selling 43.0.0.0/8"><p><br>In one of my <a href="https://toonk.io/aws-and-their-billions-in-ipv4-addresses/index.html">recent blog posts</a>, we looked at the ~two billion dollars worth of IPv4 assets AWS has collected over the last few years. One of the lessons from that story is that the IPv4 market is still very much alive and that folks are still happy to sell IPv4 resources and cloud providers are willing to pay big money to grow their business.</p><p>It’s always interesting to trace where all the sold IPv4 networks came from and who owned them previously. I’ve seen all kinds of entities sell their IPv4 resources. Examples include local police departments, libraries, car manufactures, and even large telecom hardware vendors. So I’m not sure if there’s a real pattern, other than those folks that received a large allocation decades ago and no longer need it, so it’s better to sell it.</p><p>One of the recent large transfers that caught my attention was the sale of (the majority of) the 43.0.0.0/8 IPv4 space. Since it’s such a large range and the story behind it is a bit unique, let’s take a closer look.</p><h3 id="the-asia-pacific-internet-development-trust-apidt-">The Asia Pacific Internet Development Trust (APIDT)</h3><p>In March of last year (2020), APNIC and the Japanese WIDE project announced the establishment of the Asia Pacific Internet Development Trust (APIDT). The announcement on <a href="https://blog.apnic.net/2020/03/25/announcing-the-asia-pacific-internet-development-trust-apidt/" rel="noopener">the APNIC blog </a>reads:</p><blockquote>APIDT was created in line with the wishes of the Founder of the <a href="http://www.wide.ad.jp/" rel="noreferrer noopener noopener">WIDE Project</a>, Professor Jun Murai of Keio University, following his decision on the future use of IPv4 address space in the 43/8 block, <a href="https://blog.apnic.net/2020/03/25/announcement-regarding-ipv4-address-block-43-8/" rel="noreferrer noopener noopener">announced today</a>.</blockquote><p>The article then explains the purpose of the <a href="https://www.apidt.org/" rel="noopener">Trust</a>.</p><blockquote>The Trust will use the proceeds of the sale of the address space to create a fund to benefit Internet development in the Asia Pacific region. This will include funding technical skills development and capacity building, improvements to critical Internet infrastructure, supporting research and development, and improving the community’s capability to build an open, global, stable and secure Internet.</blockquote><p>So APIDT, a joint initiative of the WIDE Project and APNIC, is a newly created Trust that will be funded by selling the unallocated (87.5%) portion of 43.0.0.0/8. The proceeds of the sale will be used in support of Internet development in the APNIC region.</p><h3 id="selling-the-43-8-addresses-space">Selling the 43/8 addresses space</h3><p>APIDT has recently completed the sale of the IPv4 address holdings in three separate stages via a sale by tender process. The IPv4 blocks sold in each stage were as follows:</p><p>Stage 1 offered the 43.0.0.0/9 block, subdivided into eight /12 blocks.<br>Stage 2 offered the 43.128.0.0/10 block, subdivided into sixteen /14 blocks.<br>Stage 3 offered the 43.192.0.0/11 block, subdivided into thirty-two /16 blocks.</p><p>The whole process took a while, the results of stage 1 and stage 2 were publicly visible in the RIR database last year (2020). For whatever reason, stage 3 took quite a bit longer and is still not fully public.</p><h3 id="so-who-bought-all-the-ipv4-addresses">So who bought all the IPv4 addresses?</h3><p>The lucky winners and, likely, big spenders are perhaps not surprisingly some of the major cloud providers.</p><h4 id="stage-1-43-0-0-0-9-block-subdivided-into-eight-12-blocks">Stage 1 - 43.0.0.0/9 block, subdivided into eight /12 blocks</h4><p>The big winner of the first stage of the tender was Alibaba. They now own all of the eight /12 blocks. These have now been allocated as two /10 to Alibaba, 43.0.0.0/10 and 43.64.0.0/10.</p><h4 id="stage-2-43-128-0-0-10-block-subdivided-into-sixteen-14-blocks">Stage 2 — 43.128.0.0/10 block, subdivided into sixteen /14 blocks</h4><p>Again a winner takes all result here. The winner of stage two is the other major cloud player in the Asia Pacific region: Tencent Cloud Computing.</p><p>The sixteen /14 blocks are now allocated to Tencent as one large /10 allocation: 43.128.0.0/10</p><h4 id="stage-3-43-192-0-0-11-block-subdivided-into-thirty-two-16-blocks">Stage 3 — 43.192.0.0/11 block, subdivided into thirty-two /16 blocks</h4><p>This one took a while but as of June 21st, 2021 we can see that just over 50% of this block, 43.192.0.0/12,  has been allocated.</p><p>The first half, 43.192.0.0/12 goes to.. surprise surprise, AWS</p><p>The final two /16’s in this block are going to<br>43.222.0.0/16 NEC Corporation<br>43.223.0.0/16 LINE Corporation</p><!--kg-card-begin: html--><h3 id="update-heading1">Update: May 23, 2022</h3><!--kg-card-end: html--><p>All of 43.192.0.0/11, with the exception of 43.222.0.0/16 (NEC Corporation) and 43.223.0.0/16 (LINE Corporation) has been allocated to AWS.<br><br>Interestingly there already was some movement around the first few /16 blocks. These used to be allocated to AWS. However, double-checking this today again, I noticed that some of these are now registered to companies that go by the name of "Sinnet Technology" and "Ningxia West Cloud Data". After some more digging, it turns out that these are the companies that operate AWS in China</p><p>From the AWS website: <a href="https://www.amazonaws.cn/en/about-aws/china/faqs/">https://www.amazonaws.cn/en/about-aws/china/faqs/</a> <br><em>Beijing Sinnet Technology Co., Ltd.("Sinnet") operates and provides services from the Amazon Web Services China (Beijing) Region, and Ningxia Western Cloud Data Technology Co., Ltd. (“NWCD”) operates and provides services from the Amazon Web Services China (Ningxia) Region, each in full compliance with Chinese regulations.</em></p><p>For a full list of what IPv4 block was sold to whom, also see this Google Sheet:<br><a href="https://docs.google.com/spreadsheets/d/13Kzg8pIJKiIV33WsRwEjvVD9myhDiAFZvOlZ1eGyW4s/edit?usp=sharing" rel="nofollow noopener">https://docs.google.com/spreadsheets/d/13Kzg8pIJKiIV33WsRwEjvVD9myhDiAFZvOlZ1eGyW4s/edit?usp=sharing</a></p><h3 id="how-much-money-was-made">How much money was made?</h3><p>That question is hard to answer as the dollar figures for the winning bids have not been made public (as far as I know). However, we can make an estimate based on the known market prices.</p><p>There’s limited public information on what price IPv4 addresses are sold for. Typically the per IP address depends on the size of the IP ranges that are being sold. One public benchmark is the <a href="https://toonk.io/aws-and-their-billions-in-ipv4-addresses/index.html" rel="noopener">transaction between AMPR.org and AWS</a> in 2019. In that transaction, AWS paid $108 Million for 44.192.0.0/10. That’s $25.74 per IP address.</p><p>A more recent public source is the <a href="https://ipv4marketgroup.com/ipv4-pricing/" rel="nofollow noopener noopener">ipv4marketgroup</a> website. According to them, the per IP price for IPv4 blocks of size of /17 or larger (/16, /15, etc) currently range between $38 to $40 USD per IP. The graph below shows how prices have steadily increased over the years.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1600/0*Znt2CImBSvT1WrQJ.jpg" class="kg-image" alt="IPv4 for sale - WIDE and APNIC selling 43.0.0.0/8"><figcaption>source: <a href="https://ipv4marketgroup.com/ipv4-pricing/" data-href="https://ipv4marketgroup.com/ipv4-pricing/" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://ipv4marketgroup.com/ipv4-pricing/</a></figcaption></figure><p><br>APIDT sold off the equivalent of 14,680,064 IPv4 addresses. So if we estimate the price at say $38 per IP, the total value of the sold IP addresses would be around $558  Million. While on the higher end, at $40 per IP the value would be $587 Million. <br>At a more conservative price of $34 per IP, the combined value would still be $499 Million (USD).</p><!--kg-card-begin: html--><h3 id="update-heading2">Update: May 23, 2022</h3><!--kg-card-end: html--><p>As of February 2022, APIDT has filed its first financial report to the Australian charities regulator, <a href="https://acncpubfilesprodstorage.blob.core.windows.net/public/4d2f7b23-e29f-ea11-a812-000d3ad1f29c-0ee7510b-a6c6-4a58-a421-0a5ba8bf6f6d-Financial%20Report-77e363ba-4698-ec11-b400-00224892dbd9-Asia_Pacific_Internet_Development_Trust_-_2021_Financial_Report_(AUD).pdf">here</a> (see page 20)</p><p>This annual financial report shows that the total amount raised with the sale of  the IP addresses was: US$440,716,493. This means that the per IP address price was almost exactly $30 (USD) </p><h3 id="wrapping-up">Wrapping up</h3><p>The sale of the 43/8 block shows the IPv4 market is still alive and thriving. We observe the market continues to find new resources to sell. In the example of APIDT, we now know that the funds made available for the Trust as a result of the sale was 440 million (US) dollars. I’d say that’s a very healthy fund for the purpose of supporting Internet development in the Asia Pacific region.</p>]]></content:encoded></item><item><title><![CDATA[The Risks and Dangers of Amplified Routing Loops.]]></title><description><![CDATA[In this article will take a closer look at network loops and how they can be abused as part of DDoS attacks. ]]></description><link>http://toonk.io/the-risks-and-dangers-of-amplified-routing-loops/</link><guid isPermaLink="false">60734b9ccac41d3ea2eb3dd7</guid><category><![CDATA[ddos]]></category><category><![CDATA[networking]]></category><dc:creator><![CDATA[Andree Toonk]]></dc:creator><pubDate>Tue, 13 Apr 2021 12:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1612908088475-da9ef8be72f6?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDV8fGNhbmFkYSUyMHBsYWNlfGVufDB8fHx8MTYxODM2OTIzMQ&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<blockquote>This article will take a closer look at network loops and how they can be abused as part of DDoS attacks. Network loops combined with existing reflection-based attacks can create a traffic amplification factor of over a thousand. In this article, we’ll see how an attacker will only need 50mb/s to fill up a 100gb/s link. I'll demonstrate this in a lab environment.</blockquote><img src="https://images.unsplash.com/photo-1612908088475-da9ef8be72f6?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDV8fGNhbmFkYSUyMHBsYWNlfGVufDB8fHx8MTYxODM2OTIzMQ&ixlib=rb-1.2.1&q=80&w=2000" alt="The Risks and Dangers of Amplified Routing Loops."><p><em><strong>This blog is also a call to action for all network engineers to clean up those lingering network loops as they aren’t just bad hygiene but a significant operational DDoS risk.</strong></em></p><h3 id="network-loops">Network Loops</h3><p>All network engineers are familiar with network loops. A network loop causes an individual packet to bounce around in a network while consuming valuable network resources (bandwidth and PPS). There are various reasons IP networks can have loops, typically caused by configuration mistakes. The only real “protection” against network loops is the Time To Live (TTL) check. However, the Time To Live check is less of protection against loops, and more protection against them looping forever.</p><p>The Time to Live (TTL) refers to the amount of time or “hops” that a packet is set to exist inside a network before being discarded by a router. The TTL is an 8-bit field in the IP header, and so it has a maximum value of 255. The typical TTL value on most operating systems is 64, which should work fine in most cases.</p><p>Most network engineers know loops are bad hygiene. I think it’s much less understood (or thought about) what the operational risk of a loop is. In my experience, most loops are for IP addresses that aren’t actually in use (otherwise, it would be an outage), so typically, solving this ends up on the backburner.</p><h4 id="a-simple-loop-example">A Simple Loop example</h4><p>Let’s look at a simple example. The diagram below shows two routers; imagine those being your core or edge routers in your datacenter. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://toonk.io/content/images/2021/04/Screen-Shot-2021-04-11-at-1.18.16-PM.png" class="kg-image" alt="The Risks and Dangers of Amplified Routing Loops." srcset="http://toonk.io/content/images/size/w600/2021/04/Screen-Shot-2021-04-11-at-1.18.16-PM.png 600w, http://toonk.io/content/images/size/w1000/2021/04/Screen-Shot-2021-04-11-at-1.18.16-PM.png 1000w, http://toonk.io/content/images/2021/04/Screen-Shot-2021-04-11-at-1.18.16-PM.png 1248w" sizes="(min-width: 720px) 720px"><figcaption>Typical loop</figcaption></figure><p>For whatever reason, they both believe 192.0.2.0/24 is reachable via the other router. As a result, packets for that destination will bounce between the two routers. Depending on the TTL value, the bandwidth used for that one packet will be:</p><pre><code>packet size in bytes x 8 x TTL.</code></pre><p>Meaning for a 512 byte packet and a TTL of 60, the amount bandwidth used is 245Kbs. In other words, the 512 byte packet turned into 307,20 bytes (60x) before it was discarded. You could think of this number 60 as the amplification factor.</p><h3 id="but-but-network-loops-are-rare">But.. but.. Network loops are rare</h3><p>That depends on your definition of rare. The good folks at <a href="https://radar.qrator.net/" rel="noopener">Qrator</a> monitor for loops on the Internet. According to the Qrator measurements, there are roughly twenty million unique loops (measured as unique router pairs).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1600/1*s0AUKUQoHReU1s53r4AGaw.png" class="kg-image" alt="The Risks and Dangers of Amplified Routing Loops."><figcaption>source: Qrator Radar. Number of routing loops&nbsp;globally</figcaption></figure><h3 id="combining-loops-and-common-amplification-attacks-">Combining loops and common Amplification attacks.</h3><p>Now that we’ve covered the risk of loops and their potential &gt; 100x (the TLL) amplification factor, this may remind you of the traditional DDoS attacks.</p><p>The record high DDOS attacks you read about in the news are now hitting hundreds of Gigabits per second and peaking into the Terabits. All these large volume metric attacks are mostly the same type of attack and are commonly known as amplification or reflection attacks.</p><p>They rely on an attacker sending a small packet with a spoofed source IP, which is then reflected and amplified to the attack target. The reflectors/amplifiers are typically some kind of open UDP service that takes a small request and yields a large answer. Typical examples are DNS, NTP, SSDP, and LDAP. The large attacks you read about in the news typically combine a few of these services.</p><p>By now, it may be clear what the danger of combining these two types of scenarios can be. Let’s look at an example. A typical DNS amplification query could be something like this, and RRSIG query for irs.gov</p><pre><code>dig -t RRSIG irs.gov @8.8.8.8</code></pre><p>This query is 64 bytes on the wire. The resulting answer is large and needs to be sent in two packets; the first packet is 1500 bytes, the second packet 944 bytes. So in total, we have an amplification factor of (1500+944)/64 = 38.</p><pre><code>IP (tos 0x0, ttl 64, id 32810, offset 0, flags [none], 
proto UDP (17), length 64)
    192.168.0.30.57327 &gt; 8.8.8.8.53: 42548+ [1au] RRSIG? irs.gov. (36)

IP (tos 0x0, ttl 123, id 15817, offset 0, flags [+], 
proto UDP (17), length 1500)
    8.8.8.8.53 &gt; 192.168.0.30.57327: 42548 8/0/1 irs.gov. RRSIG, irs.gov. 
    RRSIG, irs.gov. RRSIG, irs.gov. RRSIG, irs.gov. RRSIG[|domain]
    
IP (tos 0x0, ttl 123, id 15817, offset 1448, flags [none], 
proto UDP (17), length 944)
    8.8.8.8 &gt; 192.168.0.30: ip-proto-17</code></pre><p><em>Note: There are many different types of amplification attacks. This is just a modest and straightforward DNS example. Also, note that common open reflectors, such as Public DNS resolvers typically have smart mechanisms to limit suspicious traffic to reduce the negative impact these services could have.</em></p><p>The tcpdump output above shows that when the answer arrives back from Google’s DNS service, the TTL value is 123; this is higher than most other public DNS resolvers (most appear to default to 64).</p><p>If we combine this attack with the ‘loop’ factor we looked at previously (determined by the TTL value), we have the total amplification factor.</p><h3 id="adding-up-the-numbers">Adding up the numbers</h3><p>Ok, so let’s continue to work on the DNS amplification example. The amplification number of 38 and a TTL of 123, would result in a total amplification number of :</p><pre><code>38 * (123 / 2) = 2,337</code></pre><p><em>Note that I’m dividing the TTL number by two so that we get a per receive (RX) and transmit(tx) number.</em></p><p>For now, let’s use 2,337 as a reasonable total amplification number. What kind of traffic would an attacker need to generate 10G or 100G of traffic? One would need just about 5Mbs/s to saturate a 10g link and say ~50Mbs to saturate a 100Gbs link! These numbers are low enough to generate from a simple home connection. Now imagine what an attacker with bad intentions and access to a larger botnet could do…</p><h3 id="let-s-double-check-this-with-a-demo">Let’s double-check this with a Demo</h3><p>To make sure this is indeed all possible and the math adds up, I decided to build a lab to reproduce the scenario we looked at above.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1600/1*8CuFLCPtV9POhazPmjx2Dw.png" class="kg-image" alt="The Risks and Dangers of Amplified Routing Loops."><figcaption>Sequence of&nbsp;events</figcaption></figure><p>The lab contains four devices:</p><ol><li>An <strong>attacker:</strong> initiating a DNS-based reflection attack. The (spoofed) source IP is set to 192.0.2.53</li><li>A <strong>DNS resolver:</strong> receiving the DNS queries (with a spoofed source IP) and replying with an answer that is 38 times larger than the original question. The IP TTL value in the DNS answer is 123.</li><li>A <strong>router pair </strong>(rtr1 — rtr2): Both routers have a route for 192.0.2.0/24 pointing to each other. As a result, the DNS answer with a destination IP of 192.0.2.53 will bounce back and forth between the two routers until the TTL expires.</li></ol><figure class="kg-card kg-image-card kg-width-wide"><img src="https://cdn-images-1.medium.com/max/2400/1*rBJV8ku4B8Zk93WK0S_8mw.png" class="kg-image" alt="The Risks and Dangers of Amplified Routing Loops."></figure><p>In the screenshot above, we see the attacker on the top left sending queries at a rate of 5.9Mbs. On the bottom left, we see the DNS resolver receiving traffic at 5.9Mbs from the client and answering the queries at a rate of ~173mbs. The IP packets with the DNS responses have a TTL of 123.</p><p>We see the router pair on the right-hand side: rtr1 on the top right and rtr2 on the bottom right. As you can see, both devices are sending and receiving at 10Gb/s. So, in this case, we observe how the client (attacker) turned 6mb/s into 10Gb/s.</p><h3 id="wrapping-up">Wrapping up</h3><p>In this blog, we looked at the danger of network loops. I hope it’s clear that loops aren’t just a hygiene or cosmetic issue but instead expose a significant vulnerability that should be cleaned up ASAP.</p><p>We saw that loops are by no means rare and that there are millions of router pairs with network loops. In fact, according to Qrator data, over 30% of all Autonomous Systems (ASns), including many of the big cloud providers, have networks with loops in them.</p><p>We observed that an attacker can easily saturate a 10G link with 85Mbs (at a TTL of 240) without any UDP amplification. Or if combined with a typical UDP amplification attack, 6Mbs of seed traffic will result in 10G on a looped path, or 60Mb/s could potentially fill up a 100Gbs path!</p><h4 id="not-all-loops-are-the-same">Not all loops are the same</h4><p>Most loops happen between two adjacent routers; quite a few of those appear to occur between an ISP’s router and the customer router. I have also seen loops happen involving up to eight hops (routers) spanning various metro areas while looping between Europe and the US. These transatlantic loops are expensive and hard to scale up quickly. As a result, loops on links like these will have a more significant impact.</p><h4 id="call-to-action">Call to Action</h4><p>I hope this article convinced you to check your network for loops and make sure you won’t be affected by attacks like these. Consider signing up for the free <a href="https://radar.qrator.net/" rel="noopener">Qrator</a> service, and you’ll get alerted when new loops (or other issues) are detected in your network.</p>]]></content:encoded></item><item><title><![CDATA[Introducing SSH zero trust, Identity aware TCP sockets]]></title><description><![CDATA[<blockquote><em><em>In this article, we’ll look at Mysocket’s zero-trust <strong><strong><em>cloud-delivered, authenticating firewall. </em></strong></strong><em>Allowing</em><strong><strong><em> </em></strong></strong>you<strong><strong><em> to replace your trusted IP ranges with trusted identities.</em></strong></strong></em></em></blockquote><figure class="kg-card kg-embed-card"><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FAJ_4iHfoakY%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DAJ_4iHfoakY&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FAJ_4iHfoakY%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" allowfullscreen frameborder="0" height="480" width="640" title="Mysocket Demo - Introducing SSH zero trust, Identity aware TCP sockets" class="by fo fl kr w" scrolling="auto" style="box-sizing: inherit; width: 680px; position: absolute; left: 0px; top: 0px; height: 510px;"></iframe></figure><p>Last month we introduced our first zero trust features by introducing the concept of <em><em>Identity Aware Sockets</em></em>. It’s been great to see folks giving this</p>]]></description><link>http://toonk.io/introducing-ssh-zero-trust-identity-aware-tcp-sockets/</link><guid isPermaLink="false">602c13657c52791b41798673</guid><category><![CDATA[mysocket]]></category><category><![CDATA[networking]]></category><dc:creator><![CDATA[Andree Toonk]]></dc:creator><pubDate>Tue, 16 Feb 2021 18:57:54 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1578223003535-35db3073ada7?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDg4fHx2YW5jb3V2ZXIlMjBwZW9wbGV8ZW58MHx8fA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<blockquote><em><em>In this article, we’ll look at Mysocket’s zero-trust <strong><strong><em>cloud-delivered, authenticating firewall. </em></strong></strong><em>Allowing</em><strong><strong><em> </em></strong></strong>you<strong><strong><em> to replace your trusted IP ranges with trusted identities.</em></strong></strong></em></em></blockquote><figure class="kg-card kg-embed-card"><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FAJ_4iHfoakY%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DAJ_4iHfoakY&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FAJ_4iHfoakY%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" allowfullscreen frameborder="0" height="480" width="640" title="Mysocket Demo - Introducing SSH zero trust, Identity aware TCP sockets" class="by fo fl kr w" scrolling="auto" style="box-sizing: inherit; width: 680px; position: absolute; left: 0px; top: 0px; height: 510px;"></iframe></figure><img src="https://images.unsplash.com/photo-1578223003535-35db3073ada7?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MXwxMTc3M3wwfDF8c2VhcmNofDg4fHx2YW5jb3V2ZXIlMjBwZW9wbGV8ZW58MHx8fA&ixlib=rb-1.2.1&q=80&w=2000" alt="Introducing SSH zero trust, Identity aware TCP sockets"><p>Last month we introduced our first zero trust features by introducing the concept of <em><em>Identity Aware Sockets</em></em>. It’s been great to see folks giving this a spin and start using it as a remote access alternative for the traditional VPN.</p><p>Most services out there today are HTTP based, typically served over HTTPS. However, there are a few other commonly used services that are not HTTP based and, as a result, up until today, didn’t benefit from our identity-aware sockets. In this article, we’ll introduce Zero trust support for non-HTTP based service, with the introduction Identity aware TCP sockets. Specifically, we’ll look at providing zero trust services for SSH as an example.</p><h1 id="determining-the-user-s-identity-authentication-and-authorization">Determining the user’s identity, authentication, and authorization</h1><p>Turning your mysocket services into an identity-aware socket is as simple as adding the — cloud_authentication flag to mysocketctl when creating the service. While doing so, you have the ability to add a list of email domains and/or a list of email addresses. Now each time a user tries to access your service, a browser will pop up asking the user to authenticate. Once authentication is finished, we know the user’s identity, and if that identity matches the list of authorized users, only then will the user be let through.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://toonk.io/content/images/2021/02/Screen-Shot-2021-02-15-at-10.25.14-AM.png" class="kg-image" alt="Introducing SSH zero trust, Identity aware TCP sockets" srcset="http://toonk.io/content/images/size/w600/2021/02/Screen-Shot-2021-02-15-at-10.25.14-AM.png 600w, http://toonk.io/content/images/size/w1000/2021/02/Screen-Shot-2021-02-15-at-10.25.14-AM.png 1000w, http://toonk.io/content/images/size/w1600/2021/02/Screen-Shot-2021-02-15-at-10.25.14-AM.png 1600w, http://toonk.io/content/images/size/w2400/2021/02/Screen-Shot-2021-02-15-at-10.25.14-AM.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Creating an Identity-aware TCP socket</figcaption></figure><p>If you think about what is happening here, you’ll realize that what we have here is a <strong><strong><em><em>per session, authenticating firewall</em></em></strong></strong>. Only after the user is authenticated and authorized do we allow the network traffic through. Notice that this is much more advanced than your traditional firewall; now, <strong><strong>every network flow has an identity</strong></strong>. That’s powerful!</p><p>This flow of redirecting users to authenticate and then back to the service was do-able because it’s done in the browser and largely built of HTTP session management. Now we’d like to extend this with non-HTTP services, so we’ll need to find an alternative for the HTTP session part.<br>The solution for this comes with the help of Mutual TLS (MTLS). MTLS forces the client to authenticate itself when talking to the server. This is achieved by presenting a signed client certificate to the server.</p><h1 id="identity-aware-tcp-sockets">Identity aware TCP sockets</h1><p>With the introduction of identity-aware TCP sockets, the mysocket edge proxies act as an authenticating firewall. Since we are relying on client TLS certificates, all traffic is securely tunneled over a TLS connection.</p><p>As you can see in the flow below, there are a few actions to take before the user can get through. To make this a seamless experience for the users of your service, we’ve extended the mysocketctl command-line tool with the required functionality that kicks of the authentication flow. It starts the authentication process; after that, it requests a client certificate (your ticket in), and then it sets up the TLS tunnel for you. After that, users can send traffic over this authenticated and encrypted tunnel. In its simplest form, it will look something like this:</p><pre><code class="language-bash">echo "hello" | mysocketctl client tls \ 
    --host muddy-pond-7106.edge.mysocket.io</code></pre><p>In the example above, we’re sending the string hello, to the service served by muddy-pond-7106.edge.mysocket.io.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://toonk.io/content/images/2021/02/Screen-Shot-2021-02-15-at-10.07.49-AM.png" class="kg-image" alt="Introducing SSH zero trust, Identity aware TCP sockets" srcset="http://toonk.io/content/images/size/w600/2021/02/Screen-Shot-2021-02-15-at-10.07.49-AM.png 600w, http://toonk.io/content/images/size/w1000/2021/02/Screen-Shot-2021-02-15-at-10.07.49-AM.png 1000w, http://toonk.io/content/images/size/w1600/2021/02/Screen-Shot-2021-02-15-at-10.07.49-AM.png 1600w, http://toonk.io/content/images/2021/02/Screen-Shot-2021-02-15-at-10.07.49-AM.png 1932w" sizes="(min-width: 720px) 720px"><figcaption>Traffic flow</figcaption></figure><p>Before the string “hello” arrives at the service protected by mysocket, the mysocketctl client will take care of the authentication flow, requests the TLS client certificate, and sends whatever comes in over stdin to the mysocket edge services.</p><h1 id="ssh-zero-trust">SSH zero trust</h1><p>Now that we understand the high-level flow let’s look at a more practical example. In this example, we have a server for which I’d like to make the SSH service available to only a subset of users. The ssh is on a private network such as your corporate network, your home network, or even in a private VPC or just firewalled off from the Internet.</p><p>First, we’ll provision the service using mysocketctl on the server-side and set up the tunnel.</p><pre><code class="language-bash">mysocketctl connect \  
	--name 'remote access for my ssh server' \ 
    --cloudauth \  
    --allowed_email_addresses'contractor@gmail.com,john@example.com' \ 
    --allowed_email_domains 'mycorp.com' \ 
    --port 22 --host localhost \ 
    --type tls</code></pre><p>In this example, I'm creating a mysocket service of type TLS, and we enable cloud authentication. This will force the user to present a valid client TLS certificate. The certificate can only ever be handed out to users that authenticate with a mycorp.com email address or using the specific email addresses contractor@gmail.com or john@example.com.<br>The same command will also set up a secure tunnel to the closest mysocket tunnel servers and expose the ssh service running on port 22.</p><figure class="kg-card kg-image-card"><img src="http://toonk.io/content/images/2021/02/Screen-Shot-2021-02-13-at-1.34.56-PM.png" class="kg-image" alt="Introducing SSH zero trust, Identity aware TCP sockets" srcset="http://toonk.io/content/images/size/w600/2021/02/Screen-Shot-2021-02-13-at-1.34.56-PM.png 600w, http://toonk.io/content/images/size/w1000/2021/02/Screen-Shot-2021-02-13-at-1.34.56-PM.png 1000w, http://toonk.io/content/images/size/w1600/2021/02/Screen-Shot-2021-02-13-at-1.34.56-PM.png 1600w, http://toonk.io/content/images/2021/02/Screen-Shot-2021-02-13-at-1.34.56-PM.png 2372w" sizes="(min-width: 720px) 720px"></figure><p>The result is that this SSH service is now accessible to allowed users only, as crimson-thunder-8434.edge.mysocket.io:38676<br>Only inbound traffic with a valid client TLS ticket will be let through. Valid TLS client certificates will only ever be issued to users with a mycorp.com domain or the two contractor email addresses we specified.</p><h2 id="setting-up-an-ssh-session">Setting up an SSH session</h2><p>Ok, time to test this and connect to this ssh service. Remember that we need a valid TLS client certificate. These are issued only with a valid token, and the token is only handed out to authorized users. To make all of this easier, we’ve extended the mysocketctl tool to take care of this workflow. The example below shows how we use the ssh ProxyCommand to make that easier for us, like this.</p><pre><code class="language-bash">ssh ubuntu@crimson-thunder-8434.edge.mysocket.io \ 
	-o 'ProxyCommand=mysocketctl client tls --host %h'</code></pre><p>This will tell ssh to send all ssh traffic through this <em><em>mysocketctl client </em></em>command. This will start the authentication process, fetch the TLS client certificate for us, set up the TLS tunnel to the mysocket edge server, and transport the ssh traffic through this authenticated tunnel. The user can now log in to the ssh server using whatever method you’re used to.</p><p>With this, we’ve made our private ssh server accessible from the Internet, while the <strong><strong><em><em>authenticating mysocket firewall</em></em></strong></strong> is only allowing in session from client identities we approved beforehand. No VPN needed. Pretty cool, right?</p><h1 id="mysocket-ssh-certificate-authorities-">Mysocket SSH Certificate authorities.</h1><figure class="kg-card kg-image-card"><img src="http://toonk.io/content/images/2021/02/but-wait-theres-more--1-.jpg" class="kg-image" alt="Introducing SSH zero trust, Identity aware TCP sockets"></figure><p>SSH is quite similar to TLS in terms of workflow. It too supports authenticating users using signed certificates.</p><p>So we decided to expand on this functionality. In addition to an API endpoint that is responsible for signing TLS certificates, we also created one for signing SSH keys.</p><p>If we build on the example above, the user can now, in addition to requesting a TLS client certificate, also request a signed SSH certificate. Our SSH certificate signing service will only sign the signing request if the user is authenticated and authorized, using the same logic as before.</p><h2 id="setting-up-the-server">Setting up the server</h2><p>In order to use this, we’ll need to make a few minor changes to the SSH server. The configuration changes below are needed to enable authentication using CA keys.</p><pre><code class="language-bash">echo "TrustedUserCAKeys /etc/ssh/ca.pub" &gt;&gt;/etc/ssh/sshd_config

echo "AuthorizedPrincipalsFile %h/.ssh/authorized_principals" &gt;&gt;/etc/ssh/sshd_config

echo "mysocket_ssh_signed" &gt; /home/ubuntu/.ssh/authorized_principals</code></pre><p>Finally, also make sure to get the Public key for the CA (<em><em>mysocketctl socket show)</em></em> and copy it into the ca.pub file (<code>/etc/ssh/ca.pub)</code></p><p>Now the server is configured to work with and allow authentication based on signed SSH keys from the mysocket certificate authority. Note that all signed certificates will have two principles, the email address of the authenticated user, as well as ‘<em><em>mysocket_ssh_signed</em></em>’. In the example configuration above, we told the server to map users with the principle ‘<em><em>mysocket_ssh_signed</em></em>’ to the local user ubuntu.</p><p>Now we’re ready to connect, but instead of making the ssh command even longer, I’m going to add the following to my ssh config file ~/.ssh/config</p><pre><code>Host *.edge.mysocket.io
    ProxyCommand bash -c 'mysocketctl client ssh-keysign --host %h; ssh -tt -o IdentitiesOnly=yes -i ~/.ssh/%h %r@%h.mysocket-dummy &gt;&amp;2 &lt;&amp;1'

Host *.mysocket-dummy
    ProxyCommand mysocketctl client tls --host %h</code></pre><p>The above will make sure that for all ssh sessions to *.edge.mysocket.io we start the authentication flow, fetch a TLS client certificate, and set up the TLS tunnel. We’ll also submit an SSH key signing request, which will result in a short-lived signed SSH certificate that will be used for authenticating the SSH user.</p><p>Now the user can just SSH like this, and the whole workflow will kick-off.</p><pre><code class="language-bash">ssh ubuntu@crimson-thunder-8434.edge.mysocket.io</code></pre><p>For those interested, the ssh certificate will end up in your ~/.ssh/ directory and will look like this.</p><pre><code>$ ssh-keygen -Lf ~/.ssh/nameless-thunder-8896.edge.mysocket.io-cert.pub
/Users/andreetoonk/.ssh/nameless-thunder-8896.edge.mysocket.io-cert.pub:
        Type: ecdsa-sha2-nistp256-cert-v01@openssh.com user certificate
        Public key: ECDSA-CERT SHA256:0u6TICEhrISMCk7fbwBi629In9VWHaDG1IfnXoxjwlg
        Signing CA: ECDSA SHA256:MEdE6L0TUS0ZZPp1EAlI6RZGzO81A429lG7+gxWOonQ (using ecdsa-sha2-nistp256)
        Key ID: "atoonk@gmail.com"
        Serial: 5248869306421956178
        Valid: from 2021-02-13T12:15:20 to 2021-02-13T12:25:20
        Principals:
                atoonk@gmail.com
                mysocket_ssh_signed
        Critical Options: (none)
        Extensions:
                permit-X11-forwarding
                permit-agent-forwarding
                permit-port-forwarding
                permit-pty
                permit-user-rc</code></pre><p>With this, users can SSH to the same server as before, but the cool thing is that the server won’t need to know any traditional known credentials for its users. Things like passwords or a public key entry in the authorized_keys file belong to the past. Instead, with the help of <em><em>mysocketctl</em></em>, the user will present a short-lived signed ssh certificate, which the server will trust.</p><p>With this, we achieved true Single Sign-on (SSO) for your SSH servers. Since the certificates are short-lived, five minutes in the past(to allow for time drift) to five minutes in the future, we can be sure that for each log-in the authentication and authorization flow was successful.</p><h1 id="give-it-a-try-yourself-using-my-ssh-server">Give it a try yourself using my SSH server</h1><p>If you’re curious about what it looks like for the user and want to give it a try? I have a test VM running on 165.232.143.236, it has firewall rules to prevent SSH traffic from the Internet, but using <em><em>mysocket</em></em>, anyone with a gmail.com account can access it. I encourage you to give it a spin, it will take you less than a minute to set up, just copy-paste the one-time setup config.</p><h2 id="onetime-setup">Onetime setup</h2><p>If you’re using a Mac laptop as your client, you’ll need the mysockectl tool which will request the short-lived certs and will setup up the TLS tunnel. To install the client just copy-paste the below (for Mac only, see download.edge.mysocket.io for other platforms).</p><pre><code class="language-bash">curl -o mysocketctl https://download.edge.mysocket.io/darwin_amd64/mysocketctl 

chmod +x ./mysocketctl
sudo mv ./mysocketctl /usr/local/bin/</code></pre><p></p><p>To make it easy to use we’ll add the following to our ssh client config file. This is a one-time setup and will make sure the ssh traffic to *.edge.mysocket.io is sent through the mysocketctl client tool.</p><pre><code class="language-bash">cat &lt;&lt;EOF &gt;&gt; ~/.ssh/config
Host *.edge.mysocket.io
 ProxyCommand bash -c ‘mysocketctl client ssh-keysign --host %h; ssh -tt -o IdentitiesOnly=yes -i ~/.ssh/%h %r@%h.mysocket-dummy &gt;&amp;2 &lt;&amp;1’
Host *.mysocket-dummy
 ProxyCommand mysocketctl client tls --host %h
EOF</code></pre><p>Now you should be able to ssh to my test server using</p><pre><code class="language-bash">ssh testuser@frosty-feather-1130.edge.mysocket.io</code></pre><p>When the browser pops up, make sure to use the “log in with Google” option, as this socket has been configured to only allow identities that have a Gmail.com email address.</p><h1 id="wrapping-up">Wrapping up</h1><p>In this post, we showed how we continued to build on our previous work with “identity-aware<em><em> sockets</em></em>”. We Introduced support for identity-aware TCP sockets, by leveraging TLS tunnels and Mutual TLS for authentication.</p><blockquote><em><em>I like to think of this as a <strong><strong><em>cloud-delivered, authenticating firewall.</em></strong></strong> With this, we can make your services available to the Internet on a very granular basis, and make sure that <strong><strong><em>each flow has an identity attached to it</em></strong></strong>. Ie, we know exactly, on a per TCP flow basis, what identity (user) is using this flow. That’s a really powerful feature when compared to a traditional firewall, where we had to allow SSH traffic from certain network ranges that were implicitly trusted. What we can now do with these identity-aware sockets is rewrite these firewall rules and <strong><strong><em>replace the trusted IP ranges with trusted identities.</em></strong></strong> This is incredibly powerful for those that need strict compliance, and need to answer things like, who (not an IP, but an identity) connected to what when.</em></em></blockquote><p>We looked at how this can be used to provide zero trust remote access to your SSH servers. And how it can be further extended by using the new SSH key signing service.</p><p>That’s it for now, I hope you found this interesting and useful. As always, if you have any questions or feedback, feel free to reach out.</p><p><a href="https://www.youtube.com/playlist?list=PLSqlOpN6fPZ1myn5we6UcOx4ll9oteG_8" rel="noopener nofollow">Hungry for more? check out all our demo’s on Youtube here</a></p>]]></content:encoded></item><item><title><![CDATA[Introducing Identity Aware Sockets: Enabling Zero Trust access for your Private services]]></title><description><![CDATA[<p><em><em>In this blog post, we’ll introduce an exciting new feature that, with the help of Mysocket, allows you to deploy your own Beyond Corp setup.</em></em></p><h1 id="what-is-zero-trust">What is Zero Trust</h1><p>The main concept behind Zero Trust is that users shouldn’t just be trusted because they are on your network.</p>]]></description><link>http://toonk.io/introducing-identity-aware-sockets-enabling-zero-trust-access-for-your-private-services/</link><guid isPermaLink="false">60007bd37c52791b41798651</guid><category><![CDATA[mysocket]]></category><dc:creator><![CDATA[Andree Toonk]]></dc:creator><pubDate>Thu, 14 Jan 2021 17:19:18 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1605672183331-d2002650cf00?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDg3fHx2YW5jb3V2ZXJ8ZW58MHx8fA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1605672183331-d2002650cf00?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MXwxMTc3M3wwfDF8c2VhcmNofDg3fHx2YW5jb3V2ZXJ8ZW58MHx8fA&ixlib=rb-1.2.1&q=80&w=2000" alt="Introducing Identity Aware Sockets: Enabling Zero Trust access for your Private services"><p><em><em>In this blog post, we’ll introduce an exciting new feature that, with the help of Mysocket, allows you to deploy your own Beyond Corp setup.</em></em></p><h1 id="what-is-zero-trust">What is Zero Trust</h1><p>The main concept behind Zero Trust is that users shouldn’t just be trusted because they are on your network. This implicit trust problem is something we typically see with, for example, corporate VPNs. With most corporate VPN’s once a user is authenticated, the user becomes part of the corporate network and, as a result, has access to many of the resources within the corporate infrastructure. In other words, once you’re on the VPN, you’re within the walls of the castle, you’re trusted, and you have lots of lateral access.</p><p>The world is changing, and the once traditional approach of trusting devices on your network are over. A few years ago, Google started the journey to their implementation of Zero trust called Beyond Corp. One of the core building blocks to get to a Zero Trust model is Identity aware application proxies. These proxies can provide strict access control on a per-application granularity, taking the users’ identity and contexts such as location and device status into account.</p><h1 id="identity-aware-proxies">Identity aware proxies</h1><p>As of today, the Mysocket proxies have support for OpenID Connect, and with that, your sockets are identity-aware. This means that mysocket users can now enable authentication for their services and provide authorization rules.</p><pre><code class="language-bash">mysocketctl connect \
   --port 3000 \
   --name "My Identity aware socket" \
   --cloudauth \
   --allowed_email_domains "mycorp.com" \
   --allowed_email_addresses "contractor1@gmail.com,john@doe.com"</code></pre><p>The example above shows how we can enable authentication by using the “<em><em>cloudauth</em></em>” CLI parameter. With <em><em>cloudauth</em></em> enabled, users will be asked to authenticate before access to your service is granted. Currently, we support authentication using Google, Facebook, Github, or locally created accounts.</p><p>The authentication flow uses OpenID connect to interact with the various Identity providers (IDP); thus, we can easily add more Identity providers in the future. We’re also looking at SAML as a second authentication flow. Please let us know if you have a need for more IDP’s or SAML, and we’ll work with you.</p><h1 id="authorizations-rules">Authorizations rules</h1><p>In addition to what the Identity Service Provider (IDP) provides, mysocket also provides two authorization rules. The <em><em>— allowed_email_domain </em></em>allows users to specify a comma-separated list of email domains. If your users are authenticating using their <em><em>mycorp.com</em></em> email address, then by adding this domain as an <em><em>allowed_email_domain</em></em>, will make sure only users with that domain have will be granted access.</p><p>Since multiple identities providers are supported, it’s easy to extend access to contractors or other 3rd party users. To provide access to external contractors that are not part of your mycorp.com domain, we can use the <em><em>allowed_email_addresses</em></em> parameter to add individual identities. This is great because now you can provide these contractors access without creating corporate accounts for them.</p><p>These are just two authorization rules; we’re planning to add more types of rules in the near future. Additional authorization rules that come to mind are, Geo-based rules (only allow access from certain countries or regions) or time of day type rules. If you require these types of rules or have suggestions for additional authorization rules, please let us know!</p><h1 id="vpn-replacement">VPN replacement</h1><p>One of the unique features of mysocket is that the origin server initiates the connection to the Mysocket edge. This means that the origin servers can be on a highly secure network that only allows outbound connections. Meaning the origins can be hosted behind strict firewall rules or even behind NAT, like for example, a Private AWS VPC. With this, your origin server remains private and hidden.</p><p>With the addition of authentication and authorization to Mysocket, we can now, on a very granular basis, provide access to your private services. Combining the secure outbound tunnel property and the identity-aware sockets, we can now look at this to provide an alternative to VPNs, while providing much more granular access to private or corporate resources.</p><figure class="kg-card kg-image-card"><img src="http://toonk.io/content/images/2021/01/Screen-Shot-2021-01-12-at-9.51.34-PM.png" class="kg-image" alt="Introducing Identity Aware Sockets: Enabling Zero Trust access for your Private services" srcset="http://toonk.io/content/images/size/w600/2021/01/Screen-Shot-2021-01-12-at-9.51.34-PM.png 600w, http://toonk.io/content/images/size/w1000/2021/01/Screen-Shot-2021-01-12-at-9.51.34-PM.png 1000w, http://toonk.io/content/images/size/w1600/2021/01/Screen-Shot-2021-01-12-at-9.51.34-PM.png 1600w, http://toonk.io/content/images/2021/01/Screen-Shot-2021-01-12-at-9.51.34-PM.png 1608w" sizes="(min-width: 720px) 720px"></figure><h1 id="example-use-case">Example use case</h1><p>Imagine a scenario where you work with a contractor that needs access to one specific private application, say an internal wiki, ticket system, or the git server in your corporate network. With the traditional VPN setup, this means we’d need to provide the contractor with a VPN account. Typically this means the contractor is now part of the corporate network, has a corporate user account, and now has access to much more than just the application needed.</p><blockquote><em>Instead, what we really want is to </em><strong>only</strong><em> provide access to the one application and be very granular in who has access. With the addition of identity-aware sockets, this is now possible.</em></blockquote><h1 id="demo-time-">Demo time!</h1><p>Alright, let’s give this a spin, demo time! In this demo, we’re making a Grafana instance that’s on a private network and behind two layers of NAT available to our employees as well as a contractor.</p><p>We’ll start by setting up the socket and tunnel using the “<em>mysocketctl</em> <em>connect</em>” command.</p><p>This works great for demo’s; for more permanent setups, it’s recommended to use “<em>mysocketctl socket create</em>” and “<em>mysocketctl tunnel create</em>” so that you have a permanent DNS name for your service.</p><pre><code class="language-bash">mysocketctl connect \
   --port 3000 \
   --name "My Identity aware socket" \
   --cloudauth \
   --allowed_email_domains "mycorp.com" \
   --allowed_email_addresses "andree@toonk.io,john@doe.com"</code></pre><p>With this, we created a socket on the mysocket.io infrastructure, enabled authentication, and provided a list of authorization rules. The same command also created the secure tunnel to the closest mysocket.io tunnel server, and we’re forwarding port 3000 on localhost to the newly created socket.</p><p>Next, we launch a Grafana container. For fun, I’m passing in my AWS cloudwatch credentials, so I can create some dashboards for my AWS resources. I’ve configured grafana for proxy authentication. Meaning it will trust mysocket.io to do the authentication and authorization. Grafana will use the HTTP headers added by mysocket to determine the user information.</p><pre><code>[auth.proxy]
enabled = true
header_name = X-Auth-Email
header_property = username
auto_sign_up = true
headers = Email:X-Auth-Email</code></pre><p>The complete example grafana.ini config file I used can <a href="https://github.com/mysocketio/examples/tree/main/2021-01-14-authentication-zero-trust-grafana" rel="noopener nofollow">be found here</a>. Now we’re ready to launch Grafana. I’m doing this from my laptop, using Docker.</p><pre><code class="language-bash">docker run -i -v grafana.ini:/etc/grafana/grafana.ini \
 -e “GF_AWS_PROFILES=default” \
 -e “GF_AWS_default_ACCESS_KEY_ID=$ACCESS_KEY_ID” \
 -e “GF_AWS_default_SECRET_ACCESS_KEY=$SECRET_ACCESS_KEY” \
 -e “GF_AWS_default_REGION=us-east-1” \
 -p 3000:3000 grafana/grafana</code></pre><figure class="kg-card kg-image-card"><img src="http://toonk.io/content/images/2021/01/Screen-Shot-2021-01-13-at-10.39.59-AM.png" class="kg-image" alt="Introducing Identity Aware Sockets: Enabling Zero Trust access for your Private services" srcset="http://toonk.io/content/images/size/w600/2021/01/Screen-Shot-2021-01-13-at-10.39.59-AM.png 600w, http://toonk.io/content/images/size/w1000/2021/01/Screen-Shot-2021-01-13-at-10.39.59-AM.png 1000w, http://toonk.io/content/images/size/w1600/2021/01/Screen-Shot-2021-01-13-at-10.39.59-AM.png 1600w, http://toonk.io/content/images/2021/01/Screen-Shot-2021-01-13-at-10.39.59-AM.png 1698w" sizes="(min-width: 720px) 720px"></figure><p>Grafana is now listening on localhost port 3000. The mysocket connection we created earlier is relaying authenticated and authorized traffic to that local socket. With that, we should now be able to test and see if we have access to Grafana.</p><h1 id="wrapping-up">Wrapping up</h1><p>In this article, we introduced identity aware sockets. We saw how Mysocket users can easily enable authentication for their HTTP(S) based sockets and how OpenID connect is used for the authentication flow to Google, Facebook, or Github (for now). We then looked at how authorization rules can be added by either matching the email domain or even a list of email addresses.</p><p>With this, it’s now easy to provide access to internal applications, from any device, any time anywhere, without the need for a VPN.</p>]]></content:encoded></item><item><title><![CDATA[Global load balancing with Kubernetes and Mysocket.io]]></title><description><![CDATA[<p>If you’re in the world of cloud infrastructure, then you’ve heard of Kubernetes. Some of you are experts already, while some of us are just learning or getting started. In this blog, we’ll introduce a mysocket controller for Kubernetes and demonstrate how easy it is to use</p>]]></description><link>http://toonk.io/global-load-balancing-with-kubernetes/</link><guid isPermaLink="false">5fe18d6451a5ff44fb64d99a</guid><category><![CDATA[mysocket]]></category><dc:creator><![CDATA[Andree Toonk]]></dc:creator><pubDate>Tue, 22 Dec 2020 06:17:15 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1541741716758-85ac18fb7c62?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDl8fHZhbmNvdXZlciUyMHNoaXB8ZW58MHx8fA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1541741716758-85ac18fb7c62?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MXwxMTc3M3wwfDF8c2VhcmNofDl8fHZhbmNvdXZlciUyMHNoaXB8ZW58MHx8fA&ixlib=rb-1.2.1&q=80&w=2000" alt="Global load balancing with Kubernetes and Mysocket.io"><p>If you’re in the world of cloud infrastructure, then you’ve heard of Kubernetes. Some of you are experts already, while some of us are just learning or getting started. In this blog, we’ll introduce a mysocket controller for Kubernetes and demonstrate how easy it is to use mysocket.io as your cloud-delivered load balancer for your Kubernetes Services. If you’re a Kubernetes user already, then it should just take a minute to get this mysocket controller setup.</p><blockquote><strong>See this video for a demo of the Mysocket.io integration with Kubernetes</strong></blockquote><figure class="kg-card kg-embed-card"><iframe width="267" height="200" src="https://www.youtube.com/embed/tFfLfI1jYi4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><h3 id="pods-deployments-and-services">Pods, Deployments, and Services</h3><p>Before we continue, let’s review some of the main Kubernetes building blocks we will be using to make this work.</p><p>A typical workload running in Kubernetes will look something like the diagram below. It typically starts with a <em>deployment</em>, in which you define how many replicas (<em>pods) you’d like</em>.</p><p>Since pods are ephemeral and can scale in and out as needed, the Pod IP addresses will be dynamic. As a result, communicating with the pods in your deployment from other Deployments would require constant service discovery, which may be challenging for certain apps. To solve this, Kubernetes has the concept of a Service. A service acts as a logical network abstraction for all the pods in a workload and is a way to expose an application running on a set of<a href="https://kubernetes.io/docs/concepts/workloads/pods/" rel="noopener"> Pods</a> as a network service.</p><p>In the diagram below the <em>service</em> is reachable via 10.245.253.152 on port 8000. The <em>Service</em> will make sure traffic is distributed over all the healthy endpoints for this service.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1455/1*_0YBbv7_koFiZgcHsbZyNg.png" class="kg-image" alt="Global load balancing with Kubernetes and Mysocket.io"></figure><h3 id="taking-your-service-global">Taking your Service Global</h3><p>Now that we know how a service makes your workload available within the cluster, it’s time to take your workload global! Kubernetes has a few ways to do this, typically using an ingress service. We’re going to use the architecture as outlined in the diagram below. We’ll use Mysocket.io to create a secure tunnel between our ‘myApp’ Service and the Mysocket cloud. From there on, it will be globally available via its anycasted edge nodes.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="https://cdn-images-1.medium.com/max/2182/1*P7oVKsk2X0xBF53Xbp3Biw.png" class="kg-image" alt="Global load balancing with Kubernetes and Mysocket.io"></figure><p>To make this work, we’ll deploy a controller pod that runs the mysocketd container. The Container Image can be found on <a href="https://hub.docker.com/r/mysocket/mysocketd/tags?page=1&amp;ordering=last_updated" rel="noopener">Docker Hub</a>, and the corresponding Dockerfile on our <a href="https://github.com/mysocketio/kubernetes_controller" rel="noopener">github repo here</a>.</p><p>The mysocketd controller does two things:</p><p>1) it subscribes to the Kubernetes API and listens for events related to <em>services</em>. Specifically, it will watch for <em>service</em> events that have the <em>annotation</em> <strong><em>mysocket.io/enabled</em></strong></p><p>If a service has the following <em>annotation</em> mysocket.io/enabled: “true” then the mysocketd app will start a thread for that service.</p><p>2) In the per service thread, using the Mysocket API, a new mysocket.io “Socket” object is created if needed. This will give the “myApp” service a public DNS name and public IP. Next, it will check with the mysocket.io API to see if a tunnel already exists; if not, then a new one is created.</p><p>Finally, the secure tunnel is established, and the “myApp” service is now globally available and benefits from the mysocket.io high-performance infrastructure.</p><h3 id="demo-time-how-to-deploy-mysocketd-to-your-cluster">Demo time. How to Deploy mysocketd to your cluster</h3><p>The easiest way to get started is to download the <a href="https://github.com/mysocketio/kubernetes_controller/blob/main/mysocketd.yaml" rel="noopener">mysocketd workload yaml </a>file from the <a href="https://github.com/mysocketio/kubernetes_controller" rel="noopener">Mysocket Kubernetes controller repository</a>  and update the following three secrets to the ones for your mysocket account (<a href="https://github.com/mysocketio/kubernetes_controller/blob/main/mysocketd.yaml#L14-L16" rel="noopener">line 14,15,16</a>).</p><pre><code>email: &lt;mysocket login email in base64&gt; 
password: &lt;mysocket password in base64&gt; 
privatekey: &lt;mysocket private ssh key in base64&gt;</code></pre><p>Then simply apply like this and you’re good to go!</p><pre><code class="language-bash">kubectl apply -f mysocketd.yaml</code></pre><p>Cool, now we have the controller running!</p><h4 id="a-simple-demo-work-load">A simple demo work load</h4><p>Next up, we’d like to make our myApp service available to the Internet by using the mysocket.io service. I’m going to use <a href="https://github.com/mysocketio/examples/blob/main/2020-12-21-kubernetes-integration/demo-deploy.yaml" rel="noopener">this deployment</a> as our demo app. The deployment consists of three pods, with a little python web app, printing its hostname.</p><p>Next up, we’ll build a service (<a href="https://github.com/mysocketio/examples/blob/main/2020-12-21-kubernetes-integration/demo-service.yaml" rel="noopener">see definition here</a>) that acts as an in-cluster load balancer for the demo workload, and we’ll request the Service to be enabled for Mysocket.io.</p><pre><code class="language-bash">$ kubectl apply -f demo-deploy.yaml

$ kubectl apply -f demo-service.yaml

$ kubectl get all -n demo-app
NAME                            READY   STATUS    RESTARTS   AGE
pod/demo-app-6896cd4b88-5jzgn   1/1     Running   0          2m22s
pod/demo-app-6896cd4b88-78ngc   1/1     Running   0          2m22s
pod/demo-app-6896cd4b88-pzbc7   1/1     Running   0          2m22s
NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/demo-service   ClusterIP   10.245.114.194   &lt;none&gt;        8000/TCP   64s
NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/demo-app   3/3     3            3           2m22s
NAME                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/demo-app-6896cd4b88   3         3         3       2m22s</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1455/1*Aw8DFTKHqtz6M77I2lKscQ.png" class="kg-image" alt="Global load balancing with Kubernetes and Mysocket.io"><figcaption><strong><em>Simply set mysocket.io/enabled to “true” and your service will be globally available.</em></strong></figcaption></figure><p>The same can be done for existing services: to do so, just edit your existing Service definition and add the “<strong>mysocket,io/enabled</strong>” <em>annotation, </em>like this:</p><pre><code class="language-yaml">kind: Service
metadata:
  annotations:
    mysocket.io/enabled: "true"</code></pre><p>After applying this change, the mysocketd controller will detect that the service has requested to be connected to the Mysocket global infrastructure. The controller will create the Socket and Tunnels as needed and start the tunnel.</p><p>A quick load balancing test shows that requests to our global mysocket endpoint are now being load-balanced over the three endpoints.</p><pre><code class="language-bash">$ for i in {1..60}; do  curl -s \ 
  https://blue-snowflake-1578.edge.mysocket.io/ &amp; ;done | sort | uniq -c | sort -n  
  
  19 Server: demo-app-6896cd4b88-pzbc7  
  20 Server: demo-app-6896cd4b88-5jzgn  
  21 Server: demo-app-6896cd4b88-78ngc  
  60</code></pre><p>Just like that, in a few seconds, your workload is globally available. Pretty neat he!? Make sure to watch the <a href="https://www.youtube.com/watch?v=tFfLfI1jYi4" rel="noopener nofollow">video recording of this demo</a> as well.</p><h3 id="wrapping-up">Wrapping up</h3><p>In this article, we looked at how we can easily connect Kubernetes Services to Mysocket.io. We saw that it was easy to use. All that is needed is to: 1) start the Mysocketd controller, and: 2) add the mysocket annotation to your Services. By doing so, we are giving the Kubernetes service a publicly reachable IP address, a TLS cert, and it instantly benefits from the mysocket.io Anycast infrastructure. Pretty powerful and easy to use.</p><p>I should add that this <a href="https://github.com/mysocketio/kubernetes_controller" rel="noopener">Mysocket Controller for Kubernetes</a> is for now just a starting point, ie. an MVP integration with Kubernetes. There are ways to improve this, both in terms of user-friendliness and high availability (i.e. more than one pod). The code for this Kubernetes integration is open source, and we’d be happy to accept improvements. Mostly it serves as an example of what’s possible and how we can continue to build on the Mysocket services.</p><p>Last but certainly not least, I’d like to thanks <a href="https://github.com/tresni" rel="noopener">Brian</a> for his code to improve the various mysocketctl client libraries. Also, a big thanks to <a href="https://github.com/btoonk" rel="noopener">Bas Toonk</a> (yes, my brother) for his help with this MVP. Most of this was his work, thanks buddy!</p><p>Finally, I hope this sparked your imagination, and you’ll all give it a whirl, and let me know your experience.</p><p><br></p><p><br></p><p><br></p>]]></content:encoded></item><item><title><![CDATA[Easy Multi-region load balancing with Mysocket.io]]></title><description><![CDATA[<p>Last week AWS had a major outage in its US-EAST1 region, lasting for most of the day, just before the big black Friday sales! Incidents like this are a great reminder of the importance of multi-region or even multi-cloud deployments for your services.</p><p>Depending on your “cloud maturity” and your</p>]]></description><link>http://toonk.io/easy-multi-region-load-balancing-with-mysocket/</link><guid isPermaLink="false">5fcd79a451a5ff44fb64d968</guid><category><![CDATA[mysocket]]></category><category><![CDATA[terraform]]></category><dc:creator><![CDATA[Andree Toonk]]></dc:creator><pubDate>Mon, 07 Dec 2020 00:42:46 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1544161519-0690a8886606?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDY2fHx8ZW58MHx8fA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1544161519-0690a8886606?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MXwxMTc3M3wwfDF8c2VhcmNofDY2fHx8ZW58MHx8fA&ixlib=rb-1.2.1&q=80&w=2000" alt="Easy Multi-region load balancing with Mysocket.io"><p>Last week AWS had a major outage in its US-EAST1 region, lasting for most of the day, just before the big black Friday sales! Incidents like this are a great reminder of the importance of multi-region or even multi-cloud deployments for your services.</p><p>Depending on your “cloud maturity” and your products’ complexity, you may already be there or just getting started. Either way, in today’s blog, we will take a look at how we can use mysocket’s load balancing features to make deployments over multi-region easier.</p><h3 id="a-global-load-balancing-service">A global load balancing service</h3><p>In earlier blogs, we looked mostly at how the mysocket.io tunnel service can help securely connect your resources that may be behind NAT and firewalls to the Internet. In this article, we’ll look at Mysocket’s global load balancing feature.</p><h3 id="three-types-of-load-balancers">Three types of load balancers</h3><p>Mysocket today supports three different types of cloud-native load balancers.</p><p><em>1) Application load balancers, for your HTTP and HTTPS services. </em><br><em>2) Network load balancer, for your TCP services.</em><br><em>3) TLS Load balancer, for your TCP services, where we take care of the encryption.</em></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://toonk.io/content/images/2020/12/Screen-Shot-2020-12-06-at-2.58.50-PM.png" class="kg-image" alt="Easy Multi-region load balancing with Mysocket.io" srcset="http://toonk.io/content/images/size/w600/2020/12/Screen-Shot-2020-12-06-at-2.58.50-PM.png 600w, http://toonk.io/content/images/size/w1000/2020/12/Screen-Shot-2020-12-06-at-2.58.50-PM.png 1000w, http://toonk.io/content/images/size/w1600/2020/12/Screen-Shot-2020-12-06-at-2.58.50-PM.png 1600w, http://toonk.io/content/images/size/w2400/2020/12/Screen-Shot-2020-12-06-at-2.58.50-PM.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Load balancer types for mysocket.io&nbsp;</figcaption></figure><blockquote><em>When deploying Mysocket with your services, you now have a new front door. It just happens to be a front door that is anycasted, and as such has many doorbells around the globe made available to you. As a result, you’re always close to your users.</em></blockquote><h3 id="demo-creating-a-multi-region-service-in-two-minutes-">Demo: Creating a multi-region service in two minutes!</h3><p>Alright, time to get building! In this demo, we’ll continue using the Gif service we built in our last blog. Just like your workloads, I think this is a “critical” service, so we need it to be deployed in multiple regions. The service will need a 100% uptime, which means that even if a region is down, the Gif service will need to be available to our users.</p><figure class="kg-card kg-embed-card"><iframe width="459" height="344" src="https://www.youtube.com/embed/N96ghctEhb4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><h3 id="infrastructure-as-code">Infrastructure as code</h3><p>In this example, we’re going to deploy the service on Digital ocean VM’s in two of its regions, New York and Toronto. We’re big fans of infrastructure as code and require our service to be deployed with a simple deploy script. So we’ll use Terraform to spin up the instances and use a cloud-init script to bootstrap the necessary work to install the required software, start the Gif service and connect it to the Mysocket global infrastructure.</p><h3 id="step-1-terraform">Step 1: terraform</h3><p>We’re using terraform to define what type VM’s we’d like, which regions we want them to be deployed in, and how many per region. For this demo we’re using just two VM’s in two regions. But I’m sure, by just looking at the terraform file, you can see how easy it is to deploy many VM’s per region by just changing the ‘count’ number. It’s also easy to add additional regions. So scaling our Gif service for the future will be easy ;)</p><h3 id="step-2-cloud-init">Step 2: cloud-init</h3><p>The VM’s we’re launching are just vanilla Ubuntu machines. So we’ll need to modify them slightly to get our software and our ‘secrets’ on the machine. To do that, we’ll use cloud-init. <br>Cloud-init allows us to define a set of tasks that need to be executed when the VM is created. In our case, we’re asking it to create a user ‘mysocket’, we’re adding the Giphy API key, and mysocket credentials as secrets to the mysocket home directory. Finally, we’re telling it to download and execute a bootstrap script.</p><h3 id="step-3-bootstrapping-and-starting-our-services-">Step 3: bootstrapping and starting our services.</h3><p>Cloud-init’s last step was to start the bootstrap script that it download from <a href="https://gist.githubusercontent.com/atoonk/37ef0062fc1d90d31e2fc87e8f12d674/raw/ee83755af5214260fc9e6ad18849e77b7c8f7781/lb-do.sh" rel="noopener">here</a>. Basically, all it does is install the two python packages (mysocketctl and giphy_client) and download and start two more programs. The first program is our Gif application, which will run on port 8000. The second program is a small python script that makes sure the VM registers and connects to the mysocket.io infrastructure.</p><p>The code to connect to mysocket is <a href="https://github.com/mysocketio/examples/tree/main/2020-12-06-multi-region" rel="noopener">available on github</a>. It’s pretty easy, and I’m sure it’s low friction to get started with, even if you have limited experience. Below are the most important parts:</p><pre><code class="language-python">#These variables were made available as secrets in cloud Init.
username = os.getenv("email")
password = os.getenv("password")
socket_id = os.getenv("socket_id")

#Login to mysocket API and get a token. We can contruct the Auth header using this token.
token = get_token(username, password)
authorization_header = {
   "x-access-token": token["token"],    
   "accept": "application/json",    
   "Content-Type": "application/json",}
   
# register the service by creating a new tunnel for this VM
tunnel = new_tunnel(authorization_header, socket_id)
# setup the tunnel to mysocket and ready to serve traffic!
ssh_tunnel(port, tunnel["local_port"], ssh_server, ssh_user)</code></pre><blockquote><em>Make sure you take a quick peak at the code, it’s pretty easy to get started with and a good “getting started” example for both terraform and cloud-init.</em></blockquote><p>And with that, we now have four VM’s in two regions. All four VM’s registered with the Mysocket service and are now serving our ‘critical’ Gif app. You can see the demo service yourself here: <a href="https://fluffy-bunny-5697.edge.mysocket.io/" rel="noopener">https://fluffy-bunny-5697.edge.mysocket.io/</a></p><figure class="kg-card kg-image-card"><img src="http://toonk.io/content/images/2020/12/Screen-Shot-2020-12-06-at-4.44.43-PM-1.png" class="kg-image" alt="Easy Multi-region load balancing with Mysocket.io" srcset="http://toonk.io/content/images/size/w600/2020/12/Screen-Shot-2020-12-06-at-4.44.43-PM-1.png 600w, http://toonk.io/content/images/size/w1000/2020/12/Screen-Shot-2020-12-06-at-4.44.43-PM-1.png 1000w, http://toonk.io/content/images/2020/12/Screen-Shot-2020-12-06-at-4.44.43-PM-1.png 1364w" sizes="(min-width: 720px) 720px"></figure><p>The example below shows how the mysocket load balancing service distributes traffic evenly over all four of the origin servers in New York and Toronto.</p><pre><code class="language-bash">$ for i in {1..20}; do  curl -s \
 https://fluffy-bunny-5697.edge.mysocket.io/ | grep Server; 
done | sort | uniq -c

   5 Server: compute-000-nyc1
   5 Server: compute-000-tor1
   5 Server: compute-001-nyc1
   5 Server: compute-001-tor1</code></pre><p></p><p>Horizontally scaling the service is easy; simply change the count number in the <a href="https://github.com/mysocketio/examples/blob/main/2020-12-06-multi-region/main.tf">terraform file,</a> or add / remove regions. Since the VM’s will call the mysocket api on boot, these new VM’s will automatically become part of the load balancing pool. Pretty neat and easy, right?!</p><h3 id="failover-scenarios">Failover scenarios</h3><p>So what happens when a server or region becomes unavailable? There are a few things that make this as painless as possible for users. First, the origin service can de-register itself; this will allow for the most graceful scenario.</p><p>For less graceful scenarios, mysocket load balancers will, after 60 seconds automatically detect that a tunnel has gone down and take the origin out of the load balancing pool. And even during this 60-second degradation before the tunnel is declared down, our load balancers will use a 10 second connect time out when connecting to origin service and automatically fail back to the remaining origins. So, all in all, failures should be hidden as much as possible from your users.</p><h3 id="wrapping-up">Wrapping up</h3><p>In this blog, we looked at how Mysocket can be used as a global load balancer, for your multi-region load deployments. In our demo, we looked at two Digital ocean regions, but this could also be over multiple AWS regions, or even Multi-cloud, with one cluster in AWS, one in Digital Ocean, and throw in some Google cloud for good measure.</p><p>We saw how Mysocket provides users with a global anycasted ingress point and provides seamless load balancing for your services. Best of all, it only took us 90 seconds to get all of this going! I guess it’s fair to say that Mysocket makes going multi-region and even multi-cloud easier.</p>]]></content:encoded></item><item><title><![CDATA[Static DNS names for your mysocket.io services (and a new gif service)]]></title><description><![CDATA[<p>In my last blog post, I announced the <a href="https://mysocket.io ">mysocket.io </a>service and demonstrated how to get started quickly. It’s been great to see people signing up and giving it a spin! Your feedback has been great, motivating, and has helped make the service better already.</p><p>Most users that gave</p>]]></description><link>http://toonk.io/static-dns-names-for-your-sockets-and-a-new-gif-service/</link><guid isPermaLink="false">5fc5be7c51a5ff44fb64d914</guid><category><![CDATA[mysocket]]></category><dc:creator><![CDATA[Andree Toonk]]></dc:creator><pubDate>Tue, 01 Dec 2020 04:14:11 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1531610373189-61a2afd654e4?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDEwfHx8ZW58MHx8fA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1531610373189-61a2afd654e4?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MXwxMTc3M3wwfDF8c2VhcmNofDEwfHx8ZW58MHx8fA&ixlib=rb-1.2.1&q=80&w=2000" alt="Static DNS names for your mysocket.io services (and a new gif service)"><p>In my last blog post, I announced the <a href="https://mysocket.io ">mysocket.io </a>service and demonstrated how to get started quickly. It’s been great to see people signing up and giving it a spin! Your feedback has been great, motivating, and has helped make the service better already.</p><p>Most users that gave mysocket a try used the <em>mysocketctl connect</em> (aka “quick connect”) feature. This is the easiest way to get started and instantly creates a global socket, great for quick testing. However, when you’re done and exit the program, the “connect” feature cleans up the socket. It’s easy to create new ones, but each time with a different name. Not surprisingly then, that a few users asked the question about sockets with static names.</p><h3 id="static-names-for-your-services">Static names for your services</h3><p>Most residential ISPs give their customers a dynamic IP address (one that changes from time to time) through DHCP. If you have a service you’d like to make available but don’t have a static IP, then this post is for you!</p><p>Perhaps you have a server at home, either a fancy setup, rack-mounted and UPS for emergency power, or perhaps just a modest Raspberry Pi. Either way, you’d like to make this available from the Internet, so you can access it when you’re not at your home network, or perhaps you are home, but your nice cooperate VPN client blocks access to your local network. Or maybe you’d like to make it available to your friend.</p><p>That’s great, but you have two challenges:</p><p>1) the server sits behind NAT, so you can’t simply connect to the server from the Internet.</p><p>2) your ISP gives you dynamic IPs, so every few days/weeks, the IP changes, so it’s hard to keep track of the IP. Ideally, you could use a static name, that no matter your dynamic IP, remains stable.</p><p>So what you’d need is a static DNS name! Good news, that’s the default on the Socket primitive.</p><h2 id="mysocket-primitives">Mysocket primitives</h2><p>Let’s take a more in-depth look at the two main primitives that make up the mysocket service: Sockets and Tunnels</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1455/1*2y4Ngw325mE8T-OPoP5OSw.png" class="kg-image" alt="Static DNS names for your mysocket.io services (and a new gif service)"><figcaption>Socket and Tunnel primitives</figcaption></figure><h3 id="a-global-socket">A Global Socket</h3><p>The first primitive we need is a <em>socket</em> object. This object is the public endpoint for your service and can be created like this:</p><pre><code>mysocketctl socket create \
	--type http \
	--name "my service at home"</code></pre><p>This, among other things, returns a static DNS name that is yours to use. Think of it as your global public endpoint for your load balancer. In this case, mysocket runs the load balancer, and it’s made highly available through our anycast setup.</p><p>In the example above, we created an HTTP/HTTPS socket. Other options are TCP and TLS sockets. In those cases, the API will return not only a static DNS name for your service but also your own dedicated static TCP port number. A typical use case for a TCP socket is making your ssh service available.</p><h3 id="tunnels">Tunnels</h3><p>The second primitive is a tunnel. A tunnel object represents your origin service and the secure connection between the origin and the mysocket global infrastructure. Let’s look at the example below.</p><pre><code>mysocketctl tunnel create \
	--socket_id 1fab407c-a49d-4c5e-8287-8b138b7549c0</code></pre><p>When creating the tunnel object, simply pass along the socket_id from the socket you’d like to be connected to.</p><h3 id="putting-it-together">Putting it together</h3><p>In summary, we can create a globally available service by first creating a socket of type HTTP/HTTPS, TCP or TLS. This returns a static name for your service and optionally a unique port number. After that, we create a tunnel that links the origin to the socket.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1455/1*6FoxMwJ7gMzk3MvjAvufpw.png" class="kg-image" alt="Static DNS names for your mysocket.io services (and a new gif service)"></figure><p>Now that we have created a Socket and Tunnel, it’s time to connect to it, spin up the dataplane, and expose the local service port. We do that using the following command:</p><pre><code>mysocketctl tunnel connect \
	--port 8000 \
	--socket_id 1fab407c-a49d-4c5e-8287-8b138b7549c0 \
	--tunnel_id 3f46a01f-ef5b-4b0c-a1ce-9a294be2be03</code></pre><p>In the example above, we only create one tunnel for the socket, but nothing stops you from creating multiple tunnels (origins) per socket. In that case, mysocket will load balance over all available tunnels.</p><h2 id="demo-time-exposing-a-local-gif-service-to-the-internet">Demo time:<br>exposing a local Gif service to the Internet</h2><p>Alright, time to look at a simple demo and get our hands dirty. For this demo, I developed a small<a href="https://gist.github.com/atoonk/0bfc784feb66ffc03541462fbc945df7#file-gif_service-py" rel="noopener"> proof of concept python web service</a> that shows a random Gif from <a href="https://giphy.com/" rel="noopener">Giphy</a> each time a visitor loads the webpage.</p><p>I’m running this service locally on a VM on my laptop. The goal is to make this service publicly available, and overcoming the two levels of NAT and my ISP that hands out dynamic IP addresses. At the end of this demo, I’m able to share a static DNS name with my users, and you’ll be able to try it!</p><p><strong><em>Watch the video below for a live demo.</em></strong></p><figure class="kg-card kg-embed-card"><iframe width="459" height="344" src="https://www.youtube.com/embed/bZh3vZLCdYM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><p>First, let’s download the demo python code and start the Gif web service</p><pre><code>wget https://gist.githubusercontent.com/atoonk/0bfc784feb66ffc03541462fbc945df7/raw/812d839f60ea7bfd98a530a3fc549137abe0b329/gif_service.py

#make sure to update the API key in gif_service.py

python3 ./gif_service.py</code></pre><p>Alright, now we have this service running, but it’s only reachable from my local network. Next up, we create a socket and a tunnel.</p><pre><code>mysocketctl socket create \
	--name "my local http Gif service" \ 
	--type http</code></pre><p>Ok, that returns a socket_id and our static DNS name! In my case, the DNS name is: <a href="https://wandering-shape-7752.edge.mysocket.io" rel="noopener">wandering-shape-7752.edge.mysocket.io</a></p><p>Next up, we’ll use the socket_id to create the tunnel object:</p><pre><code>mysocketctl tunnel create \
	--socket_id 1fab407c-a49d-4c5e-8287-8b138b7549c0</code></pre><p>Cool, now all we need to do is start the tunnel connection.</p><pre><code>mysocketctl tunnel connect \
	--port 8000 \
	--socket_id 1fab407c-a49d-4c5e-8287-8b138b7549c0 \
	--tunnel_id 3f46a01f-ef5b-4b0c-a1ce-9a294be2be03</code></pre><p>This will securely connect the local gif web service listening on port 8000, to the mysocket infrastructure and make it available as <a href="https://wandering-shape-7752.edge.mysocket.io/" rel="noopener">https://wandering-shape-7752.edge.mysocket.io</a></p><h3 id="wrapping-up">Wrapping up</h3><p>In this blog post, we looked at the two main mysocket primitives, <em>Sockets</em>, and <em>Tunnels</em>. We saw how users can create a Socket object and get a static DNS name and possibly even a dedicated TCP port. In the demo, we used the <em>tunnel connect</em> feature to make port 8000 on my local VM available to the Internet. With that, we made the Internet a little bit better by adding <a href="http://wandering-shape-7752.edge.mysocket.io/" rel="noopener">yet another Gif service</a>, that just happens to run on a VM hosted on my laptop.</p><p></p><!--kg-card-begin: html--><img src="https://media.giphy.com/media/aaw0ay7NqrucU/source.gif" alt="Static DNS names for your mysocket.io services (and a new gif service)">
<!--kg-card-end: html-->]]></content:encoded></item><item><title><![CDATA[Introducing Mysocket.io]]></title><description><![CDATA[Mysocket is a service that provides users with fast and secure TCP sockets for services that aren’t normally directly reachable from the Internet. Using a simple client, we connect your local services to the Internet.]]></description><link>http://toonk.io/introducing-mysocket-io/</link><guid isPermaLink="false">5fbb2ca251a5ff44fb64d8b7</guid><category><![CDATA[mysocket]]></category><category><![CDATA[anycast]]></category><dc:creator><![CDATA[Andree Toonk]]></dc:creator><pubDate>Mon, 23 Nov 2020 03:40:48 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1519331582073-283f1a211a3e?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<blockquote>In this blog, I’d like to introduce a new project I’m calling Mysocket.io. Before we dive in, a bit of background.</blockquote><img src="https://images.unsplash.com/photo-1519331582073-283f1a211a3e?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Introducing Mysocket.io"><p>Loyal readers know I enjoy building global infrastructure services that need to be able to carry a significant amount of traffic and a large number of requests. Building services like these often require us to solve several challenges. Things to consider include: high availability, scaling, DDoS proofing, monitoring, logging, testing, deployments, user-facing &amp; backend APIs, policy management (user preferences) and distribution, life-cycling, etc. All this while keeping an eye on cost and keeping complexity to a minimum (which really is human, operational cost).</p><p>To experiment with these topics, it’s necessary to have a project to anchor these experiments to. Something I can continuously work on, and while doing so, improve the project as a whole. Now, there are many projects I started over time, but one that I've worked most on recently, and wanted to share with a wider audience, is <a href="https://www.mysocket.io/" rel="noopener"><strong>mysocket.io</strong></a>. A service that provides secure public endpoints for services that are otherwise not publicly reachable.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1600/1*43NirbSJ49IN0yPBE5wuAg.png" class="kg-image" alt="Introducing Mysocket.io"></figure><p>A typical example case that <a href="https://mysocket.io" rel="noopener">mysocket.io</a> can help with is a web service running on your laptop, which you’d like to make available to a team member or client. Or ssh access to servers behind NAT or a firewall, like a raspberry pi on your home network or ec2 instances behind NAT.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1600/1*yF9ppVXSZcwOz5Rlzh39QQ.png" class="kg-image" alt="Introducing Mysocket.io"><figcaption>make your localhost app available from&nbsp;anywhere</figcaption></figure><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1600/1*E-cZOCwqsH3tOkRjzA-ZJQ.png" class="kg-image" alt="Introducing Mysocket.io"><figcaption>Provide SSH access to your home server behind&nbsp;NAT.</figcaption></figure><h2 id="more-details">More details</h2><p>Alright, a good way to share more details and is to do a quick demo! You can see a brief overview in this video, or even better, try it yourself by simply following the four easy steps below.</p><figure class="kg-card kg-embed-card"><iframe width="459" height="344" src="https://www.youtube.com/embed/lEZX_xCTDlo?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><blockquote>If you’re interested or curious, feel free to give it a spin and let me know what worked or didn’t, or even better, how it can be improved. Getting started will take you just one minute. Just follow these simple steps.</blockquote><pre><code class="language-bash">#Install client, using python's package manager (pip)
pip3 install mysocketctl

#Create account
mysocketctl account create \
    --name "your_name" \
    --email "your_email_address" \
    --password "a_secure_password" \
    --sshkey "$(cat ~/.ssh/id_rsa.pub)"
    
#login
mysocketctl login  \
    --email "your_email_address" \
    --password "a_secure_password" 
    
 
#Launch your first global socket ;)
mysocketctl connect \
    --port 8000 \
    --name "my test service"</code></pre><h2 id="architecture-overview">Architecture overview</h2><p>Ok, so how does it work? The process for requesting a “global socket” starts with an API call. You can do this by directly interfacing with <a href="https://api.mysocket.io/" rel="noopener">the RESTful API</a>, or by using the <a href="https://mysocket.readthedocs.io/en/latest/mysocketctl/mysocket.html" rel="noopener">mysocketctl</a> tool. This returns a global mysocket object, which has a name, port number(s), and some other information.</p><p>Users can now use this socket object to create tunnel objects. These tunnels are then used to connect your local service to the global mysocket.io infrastructure. By stitching these two TCP sessions together, we made your local service globally available.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1600/1*L4orGfToNQ3fUgjrZC_3Qw.png" class="kg-image" alt="Introducing Mysocket.io"><figcaption>Creating a Socket, a Tunnel and connecting to mysocket.io</figcaption></figure><p>The diagram below provides a high-level overview of the service data-plane. On the left, we have the origin service. This could be your laptop, your raspberry pi at home, or even a set of containers in a Kubernetes cluster. The origin service can be behind a very strict firewall or even NAT. All it needs is outbound network access. We can then set up a secure encrypted tunnel to any of the mysocket.io servers around the world.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1600/1*ePYWcZbw_jzNriOwCThNFQ.png" class="kg-image" alt="Introducing Mysocket.io"><figcaption>Mysocket.io dataplane</figcaption></figure><h2 id="anycast">Anycast</h2><p>The Mysocket.io services use AWS’ global accelerator. With this, I’m making both the tunnel servers and proxy services anycasted. This solves some of the load balancing and high availability challenges. The mysocket tunnel and proxy servers are located in North America, Europe, and Asia.</p><p>Once the tunnel is established, the connection event is signaled to all other nodes in real-time, ensuring that all edge nodes know where the tunnel for the service is.</p><h2 id="documentation">Documentation</h2><p>One of my goals is to make Mysocket super easy to use. One way to do that is to have good documentation. I invite you to check out our readthedocs.io documentation here <a href="https://mysocket.readthedocs.io/en/latest/" rel="noopener">https://mysocket.readthedocs.io/</a></p><p>It’s divided into two sections:</p><ol><li><a href="https://mysocket.readthedocs.io/en/latest/about/about.html" rel="noopener">General information </a>about mysocket.io and some of the concepts.</li><li>Information and user guides for the <a href="https://mysocket.readthedocs.io/en/latest/mysocketctl/mysocket.html" rel="noopener">mysocketctl command-line tool</a>.</li></ol><p>The documentation itself and mysocketctl tool are both <a href="https://github.com/mysocketio" rel="noopener">opensource</a> so feel free to open pull requests or open issues if you have any questions.</p><p>You may have noticed there’s a <a href="https://mysocket.io" rel="noopener">website</a> as well. I wanted to create a quick landing page, so I decided to play with Wix.com. They make it super easy;  I may have gone overboard a bit ;)  All that was clicked together in just one evening, pretty neat.</p><h2 id="more-to-come">More to come</h2><p>There’s a lot more to tell and plenty more geeky details to dive into. More importantly, we can continue to build on this and make it even better (ping me if you have ideas or suggestions)! <br>So stay tuned. That’s the plan for subsequent Blog posts soon, either in this blog or the <a href="https://www.mysocket.io/blog" rel="noopener">mysocket.io blog</a>.</p><p><em>Cheers,</em><br><em> -Andree</em><br></p>]]></content:encoded></item><item><title><![CDATA[AWS and their Billions in IPv4 addresses]]></title><description><![CDATA[<p>Earlier this week, I was doing some work on AWS and wanted to know what IP addresses were being used. Luckily for me, AWS publishes this all here <a href="https://ip-ranges.amazonaws.com/ip-ranges.json" rel="nofollow noopener">https://ip-ranges.amazonaws.com/ip-ranges.json</a>. When you go through this list, you’ll quickly see that AWS has a massive asset</p>]]></description><link>http://toonk.io/aws-and-their-billions-in-ipv4-addresses/</link><guid isPermaLink="false">5f88bf632257c9dcdf92feee</guid><category><![CDATA[ipv4]]></category><category><![CDATA[aws]]></category><category><![CDATA[networking]]></category><dc:creator><![CDATA[Andree Toonk]]></dc:creator><pubDate>Tue, 20 Oct 2020 16:01:11 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1572288717056-ea97fb77c71e?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1572288717056-ea97fb77c71e?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ" alt="AWS and their Billions in IPv4 addresses"><p>Earlier this week, I was doing some work on AWS and wanted to know what IP addresses were being used. Luckily for me, AWS publishes this all here <a href="https://ip-ranges.amazonaws.com/ip-ranges.json" rel="nofollow noopener">https://ip-ranges.amazonaws.com/ip-ranges.json</a>. When you go through this list, you’ll quickly see that AWS has a massive asset of IPv4 allocations. Just counting quickly I noticed a lot of big prefixes.</p><figure class="kg-card kg-embed-card"><blockquote class="twitter-tweet" data-width="550"><p lang="en" dir="ltr">Ever wondered what all of the AWS network ranges are? You can find them all here:<a href="https://t.co/NBaBF6w0la">https://t.co/NBaBF6w0la</a><br>That&#39;s *a lot* of big prefixes!<br>4x /11, 14x /12, 30x /13, 78x /14, 184x /15, 278x /16</p>&mdash; Andree Toonk, Adelante! (@atoonk) <a href="https://twitter.com/atoonk/status/1316098702260359168?ref_src=twsrc%5Etfw">October 13, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</figure><p>However, the IPv4 ranges on that list are just the ranges that are in use and allocated today by AWS. Time to dig a bit deeper.</p><h3 id="ipv4-address-acquisitions-by-aws">IPv4 address acquisitions by AWS</h3><p>Over the years, AWS has acquired a lot of IPv4 address space. Most of this happens without gaining too much attention, but there were a few notable acquisitions that I’ll quickly summarize below.</p><h4 id="2017-mit-selling-8-million-ipv4-addresses-to-aws">2017: MIT selling 8 million IPv4 addresses to AWS</h4><p>In 2017 <a href="https://www.internetsociety.org/blog/2017/05/mit-goes-on-ipv4-selling-spree/" rel="noopener">MIT sold half of its 18.0.0.0/8</a> allocation to AWS. This 18.128.0.0/9 range holds about 8 million IPv4 addresses.</p><h4 id="2018-ge-sells-3-0-0-0-8-to-aws">2018: GE sells 3.0.0.0/8 to AWS</h4><p>In 2018 the IPv4 prefix 3.0.0.0/8 was transferred from GE to AWS. With this, AWS became the proud owner of its first /8! That’s sixteen million new IPv4 addresses to feed us hungry AWS customers. <a href="https://news.ycombinator.com/item?id=18407173" rel="nofollow noopener">https://news.ycombinator.com/item?id=18407173</a></p><h4 id="2019-aws-buys-amprnet-44-192-0-0-10">2019: AWS buys AMPRnet 44.192.0.0/10</h4><p>In 2019 AWS bought a /10 from AMPR.org, the Amateur Radio Digital Communications (ARDC). The IPv4 range 44.0.0.0/8 was an allocation made to the Amateur Radio organization in 1981 and known as the AMPRNet. This sell caused a fair bit of discussion, check out the <a href="https://mailman.nanog.org/pipermail/nanog/2019-July/thread.html#102103" rel="noopener">nanog discussion here.</a></p><p>Just this month, it <a href="http://www.southgatearc.org/news/2020/october/sale-of-amateur-radio-amprnet-tcp-ip-addresses.htm" rel="noopener">became public knowledge</a> AWS paid $108 million for this /10. That’s $25.74 per IP address.</p><p>These are just a few examples. Obviously, AWS has way more IP addresses than the three examples I listed here. The IPv4 transfer market is very active. Check out this website to get a sense of all transfers: <a href="https://account.arin.net/public/transfer-log#NRPM-8.3IPv4" rel="noopener">https://account.arin.net/public/transfer-log</a></p><h3 id="all-aws-ipv4-addresses">All AWS IPv4 addresses</h3><p>Armed with the information above it was clear that not all of the AWS owned ranges were in the <a href="https://ip-ranges.amazonaws.com/ip-ranges.json">JSON</a> that AWS published. For example, parts of the 3.0.0.0/8 range are missing. Likely because some of it is reserved for future use.</p><p>I did a bit of digging and tried to figure out how many IPv4 addresses AWS really owns. A good start is the Json that AWS publishes. I then combined that with all the ARIN, APNIC, and RIPE entries for Amazon I could find. A few examples include:<br><br><a href="https://rdap.arin.net/registry/entity/AMAZON-4" rel="nofollow noopener">https://rdap.arin.net/registry/entity/AMAZON-4</a><br><a href="https://rdap.arin.net/registry/entity/AMAZO-4" rel="nofollow noopener">https://rdap.arin.net/registry/entity/AMAZO-4</a><br><a href="https://rdap.arin.net/registry/entity/AT-88-Z" rel="nofollow noopener">https://rdap.arin.net/registry/entity/AT-88-Z</a></p><p>Combining all those IPv4 prefixes, removing duplicates and overlaps by aggregating them results in the following list of unique IPv4 address owned by AWS: <a href="https://gist.github.com/atoonk/b749305012ae5b86bacba9b01160df9f#all-prefixes" rel="nofollow noopener">https://gist.github.com/atoonk/b749305012ae5b86bacba9b01160df9f#all-prefixes</a></p><p>The total number of IPv4 addresses in that list is just over 100 Million (100,750,168). That’s <strong>the equivalent of just over six /8’s,</strong> not bad!</p><p>If we break this down by allocation size, we see the following:</p><pre><code>1x /8     =&gt; 16,777,216 IPv4 addresses
1x /9     =&gt; 8,388,608 IPv4 addresses
4x /10    =&gt; 16,777,216 IPv4 addresses
5x /11    =&gt; 10,485,760 IPv4 addresses
11x /12   =&gt; 11,534,336 IPv4 addresses
13x /13   =&gt; 6,815,744 IPv4 addresses
34x /14   =&gt; 8,912,896 IPv4 addresses
53x /15   =&gt; 6,946,816 IPv4 addresses
182x /16  =&gt; 11,927,552 IPv4 addresses
&lt;and more&gt;</code></pre><p>A complete breakdown can be found here: <a href="https://gist.github.com/atoonk/b749305012ae5b86bacba9b01160df9f#breakdown-by-ipv4-prefix-size" rel="nofollow noopener">https://gist.github.com/atoonk/b749305012ae5b86bacba9b01160df9f#breakdown-by-ipv4-prefix-size</a></p><h3 id="putting-a-valuation-on-aws-ipv4-assets">Putting a valuation on AWS’ IPv4 assets</h3><blockquote>Alright.. this is just for fun…</blockquote><p>Since AWS is (one of) the largest buyers of IPv4 addresses, they have spent a significant amount on stacking up their IPv4 resources. It’s impossible, as an outsider, to know how much AWS paid for each deal. However, we can for fun, try to put a dollar number on AWS’ current IPv4 assets.</p><p>The average price for IPv4 addresses has gone up over the years. From ~$10 per IP a few years back to ~$25 per IP <a href="https://auctions.ipv4.global/" rel="noopener">nowadays</a>. <br>Note that these are market prices, so if AWS would suddenly decide to sell its IPv4 addresses and overwhelm the market with supply, prices would drop. But that won’t happen since we’re all still addicted to IPv4 ;)</p><p>Anyway, let’s stick with $25 and do the math just for fun.</p><pre><code>100,750,168 ipv4 addresses x $25 per IP = $2,518,754,200</code></pre><p>Just<strong> over $2.5 billion worth of IPv4 addresses,</strong> not bad! </p><h3 id="peeking-into-the-future">Peeking into the future</h3><p>It’s clear AWS is working hard behind the scenes to make sure we can all continue to build more on AWS. One final question we could look at is: <em>how much buffer does AWS have?</em> ie. how healthy is their IPv4 reserve?</p><p>According to their <a href="https://ip-ranges.amazonaws.com/ip-ranges.json" rel="noopener">published data</a>, they have allocated roughly 53 Million IPv4 addresses to existing AWS services. We found that all their IPv4 addresses combined equates to approximately 100 Million IPv4 addresses. That means they still have ~47 Million IPv4 addresses, or 47% available for future allocations. That’s pretty healthy! And on top of that, I’m sure they’ll continue to source more IPv4 addresses. The IPv4 market is still hot!</p><p></p>]]></content:encoded></item><item><title><![CDATA[100G networking in AWS, a network performance deep dive]]></title><description><![CDATA[<h3></h3><p>Loyal readers of my blog will have noticed a theme, I’m interested in the continued move to virtualized network functions, and the need for faster networking options on cloud compute. In this blog, we’ll look at the network performance on the juggernaut of cloud computing, AWS.</p><p>AWS is</p>]]></description><link>http://toonk.io/aws-network-performance-deep-dive/</link><guid isPermaLink="false">5f889c6f2257c9dcdf92feb0</guid><category><![CDATA[aws]]></category><category><![CDATA[networking]]></category><dc:creator><![CDATA[Andree Toonk]]></dc:creator><pubDate>Thu, 15 Oct 2020 19:06:42 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1571864156103-d3c37503b6af?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<h3></h3><img src="https://images.unsplash.com/photo-1571864156103-d3c37503b6af?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ" alt="100G networking in AWS, a network performance deep dive"><p>Loyal readers of my blog will have noticed a theme, I’m interested in the continued move to virtualized network functions, and the need for faster networking options on cloud compute. In this blog, we’ll look at the network performance on the juggernaut of cloud computing, AWS.</p><p>AWS is the leader in the cloud computing world, and many companies now run parts of their services on AWS. The question we’ll try to answer in this article is: how well suited is AWS’ ec2 for high throughput network functions.</p><p><em>I’ve decided to experiment with adding a short demo video to this blog. Below you will find a quick demo and summary of this article. Since these videos are new and a bit of an experiment, let me know if you like it.</em></p><figure class="kg-card kg-embed-card"><iframe width="612" height="344" src="https://www.youtube.com/embed/iwe0gL8VBvA?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><h3 id="100g-networking">100G networking</h3><p>It’s already been two years since AWS <a href="https://aws.amazon.com/about-aws/whats-new/2018/11/introducing-amazon-ec2-c5n-instances/" rel="noopener">announced the C5n instances</a>, featuring 100 Gbps networking. I’m not aware of any other cloud provider offering 100G instances, so this is pretty unique. Ever since this was released I wondered exactly what, if any, the constraints were. Can I send/receive 100g line rate (144Mpps)? So, before we dig into the details, let’s just check if we can really get to 100Gbs.</p><figure class="kg-card kg-embed-card kg-card-hascaption"><blockquote class="twitter-tweet" data-width="550"><p lang="en" dir="ltr">this is fun :)  97Gbs <a href="https://t.co/6VdkR2Rlr4">pic.twitter.com/6VdkR2Rlr4</a></p>&mdash; Andree Toonk, Adelante! (@atoonk) <a href="https://twitter.com/atoonk/status/1266037590492241921?ref_src=twsrc%5Etfw">May 28, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<figcaption>100gbs testing.. We’re gonna need a bigger boat..</figcaption></figure><p>There you have it, I was able to get to 100Gbs between 2 instances! That’s exciting. But there are a few caveats. We’ll dig into all of them in this article, with the aim to understand exactly what’s possible, what the various limits are, and how to get to 100g.</p><h3 id="understand-the-limits">Understand the limits</h3><p>Network performance on Linux is typically a function of a few parameters. Most notably, the number of TX/RX queues available on the NIC (network card). The number of CPU cores, ideally at least equal to the number of queues. The pps (packets per second) limit per queue. And finally, in virtual environments like AWS and GCP, potential admin limits on the instance.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1200/0*JUjZ5NOIWUrey-Xm" class="kg-image" alt="100G networking in AWS, a network performance deep dive"></figure><p>Doing networking in software means that processing a packet (or a batch of them) uses a number of CPU cycles. It’s typically not relevant how many bytes are in a packet. As a result, the best metric to look at is the: pps number (related to our cpu cycle budget). Unfortunately, the pps performance numbers for AWS aren’t published so, we’ll have to measure them in this blog. With that, we should have a much better understanding of the network possibilities on AWS, and hopefully, this saves someone else a lot of time (this took me several days of measuring) ;)</p><h3 id="network-queues-per-instance-type">Network queues per instance type</h3><p>The table below shows the number of NIC queues by ec2 (c5n) Instance type.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1600/1*P9_cWmValN63fmRXAWqdqg.png" class="kg-image" alt="100G networking in AWS, a network performance deep dive"></figure><p><em>In the world of ec2, 16 vCPUs on the C5n 4xl instance means 1 Socket, 8 Cores per socket, 2 Threads per core.</em></p><p>On AWS, an Elastic Network Adapter (ENA) NIC has as many queues as you have vCPUs. Though it stops at 32 queues, as you can see with the C5n 9l and C5n 18xl instance.</p><p>Like many things in computing, to make things faster, things are parallelized. We see this clearly when looking at CPU capacity, we’re adding more cores, and programs are written in such a way that can leverage the many cores in parallel (multi-threaded programs).</p><p>Scaling Networking performance on our servers is done largely the same. It’s hard to make things significantly faster, but it is easier to add more ‘workers’, especially if the performance is impacted by our CPU capacity. In the world of NICs, these ‘workers’ are queues. Traffic send and received by a host is load-balanced over the available network queues on the NIC. This load balancing is done by hashing (typically the 5 tuples, protocol, source + destination address, and port number). Something you’re likely familiar with from ECMP.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1200/1*VsV_F3dbhosQXAi9A97mqQ.jpeg" class="kg-image" alt="100G networking in AWS, a network performance deep dive"></figure><blockquote>So queues on a NIC are like lanes on a highway, the more lanes, the more cars can travel the highway. The more queues, the more packets (flows) can be processed.</blockquote><h3 id="test-one-ena-queue-performance">Test one, ENA queue performance</h3><p>As discussed, the network performance of an instance is a function of the number of available queues and cpu’s. So let’s start with measuring the maximum performance of a single flow (queue) and then scale up and measure the pps performance.</p><p>In this measurement, I used two c5n.18xlarge ec2 instances in the same subnet and the same placement zone. The sender is using <a href="https://toonk.io/building-a-high-performance-linux-based-traffic-generator-with-dpdk/index.html" rel="noopener">DPDK-pktgen</a> (igb_uio). The receiver is a stock ubuntu 20.04 LTS instance, using the ena driver.</p><p>The table below shows the TX and RX performance between the two c5n.18xlarge ec2 instances for one and two flows.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1600/1*GVmlTopDSacvRFilauWZag.png" class="kg-image" alt="100G networking in AWS, a network performance deep dive"></figure><p>With this, it seems the per queue limit is about 1Mpps. Typically the per queue limit is due to the fact that a single queue (soft IRQ) is served by a single CPU core. Meaning, the per queue performance is limited by how many packets per second a single CPU core can process. So again, what you typically see in virtualized environments is that the number of network queues goes up with the number of cores in the VM. In ec2 this is the same, though it’s maxing out at 32 queues.</p><h3 id="test-two-rx-only-pps-performance">Test two, RX only pps performance</h3><p>Now that we determined that the per queue limit appears to be roughly one million packets per second, it’s natural to presume that this number scales up horizontally with the number of cores and queues. For example, the C5n 18xl comes with 32 nic queues and 72 cores, so in theory, we could naively presume that the (RX/TX) performance (given enough flows) should be 32Mpps. Let’s go ahead and validate that.</p><p>The graph below shows the Transmit (TX) performance as measured on a c5n.18xlarge. In each measurement, I gave the packet generator one more queue and vcpu to work with. Starting with one TX queue and one VCPu, incrementing this by one in each measurement until we reached 32 vCPU and 32 queues (max). The results show that the per TX queue performance varied between 1Mpps to 700Kpps. The maximum total TX performance I was able to get however, was ~8.5Mpps using 12 TX queues. After that, adding more queues and vCPu’s didn’t matter, or actually degraded the performance. So this indicates that the performance scales horizontally (per queue), but does max out at a certain point (varies per instance type), in this case at 8.5 Mpps</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1600/0*7ATryTmrHyn_Inaj" class="kg-image" alt="100G networking in AWS, a network performance deep dive"><figcaption>c5n.18xlarge per TX queue performance</figcaption></figure><p>In this next measurement, we’ll use two packet generators and one receiver. I’m using two generators, just to make sure the limit we observed earlier isn’t caused by limitations on the packet generator. Each traffic generator is sending many thousands of flows, making sure we leverage all the available queues.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1600/1*S8jlTc-v-Krm8BLx1QCGtw.png" class="kg-image" alt="100G networking in AWS, a network performance deep dive"><figcaption>RX pps per C5N instance&nbsp;type</figcaption></figure><p>Alright, after a few minutes of reading (and many, many hours, well really days.. of measurements on my end), we now have a pretty decent idea of the performance numbers. We know how many queues each of the various c5n instance types have.</p><blockquote><strong>We have seen that the per queue limit is roughly 1Mpps. And with the table above, we now see how many packets per second each instance is able to receive (RX)</strong>.</blockquote><h3 id="forwarding-performance">Forwarding performance</h3><p>If we want to use ec2 for virtual network functions, then just receiving traffic isn’t enough. A typical router or firewall should both receive and send traffic at the same time. So let’s take a look at that.</p><p>For this measurement, I used the following setup. Both the traffic generator and receiver were C5n-18xl instances. The Device Under Test (DUT) was a standard Ubuntu 20.04 LTS instance using the ena driver. Since the earlier observed pps numbers weren’t too high, I determined it’s safe to use the regular Linux kernel to forward packets.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1600/1*rK_4B6M-0NF2mrrDmheuFQ.png" class="kg-image" alt="100G networking in AWS, a network performance deep dive"><figcaption>test setup</figcaption></figure><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1600/1*UUWrmoLH-MXLMsNBdCvJXg.png" class="kg-image" alt="100G networking in AWS, a network performance deep dive"><figcaption>pps forwarding performance</figcaption></figure><p>The key takeaway from this measurement is that the TX and RX numbers are similar as we’d seen before for the instance types up to (including) the C5n 4xl. For example, earlier we saw the C5n 4xl could receive up to ~3Mpps. This measurement shows that it can do ~3Mpps simultaneously on RX and TX.</p><p>However, if we look at the C5n 9l, we can see it was able to process RX+ TX about 6.2Mpps. Interestingly, earlier we saw it was also able to receive (rx only) ~6Mpps. So it looks like we hit some kind of aggregate limit. We observed a similar limit for the C5n 18xl instance.</p><h3 id="in-summary-">In Summary.</h3><p>In this blog, we looked at the various performance characteristics of networking on ec2. We determined that the <strong>performance of a single queue is roughly 1Mpps</strong>. We then saw how the number of queues goes up with the higher end instances up until <strong>32 queues maximum</strong>.</p><p>We then measure the RX performance of the various instances as well as the forwarding (RX + TX aggregate) performance. Depending on the measurement setup (RX, or TX+RX) we see that <strong>for the largest instance types, the pps performance maxes out at roughly 6.6Mpps to 8.3Mpps</strong>. With that, I think that the C5n 9l hits the sweet spot in terms of cost vs performance.</p><h3 id="so-how-about-that-100g-test">So how about that 100G test?</h3><p>Ah yes! So far, we talked about pps only. How does that translate that to gigabits per second?<br>Let’s look at the quick table below that shows how the pps number translates to Gbs at various packet sizes.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1600/1*sXL3CJJ9M955aa8DvRUVyw.png" class="kg-image" alt="100G networking in AWS, a network performance deep dive"></figure><p>These are a few examples to get to 10G at various packet sizes. This shows that in order to support line-rate 10G at the smallest packet size, the system will need to be able to do ~14.88 Mpps. The 366 byte packet size is roughly the equivalent average of what you’ll see with an IMIX test, for which the systems needs to be able to process ~3,4Mpps to get to 10G line rate.</p><p>If we look at the same table but then for 100gbps, we see that at the smallest packet size, an instance would need to be able to process is over 148Mpps. But using 9k jumbo frames, you only need 1.39Mpps.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1600/1*WbChAl1uTuUMnP6oHFzRhg.png" class="kg-image" alt="100G networking in AWS, a network performance deep dive"></figure><p>And so, that’s what you need to do to get to 100G networking in ec2. Use Jumbo frames (supported in ec2, in fact, for the larger instances, this was the default). With that and a few parallel flows you’ll be able to get to 100G “easily”!</p><h3 id="a-few-more-limits">A few more limits</h3><p>One more limitation I <a href="https://cloudonaut.io/ec2-network-performance-cheat-sheet/" rel="noopener">read about while researching</a>, but didn’t look into myself. It appears that some of the instances have time-based limits on the performance. This <a href="https://www.bluematador.com/blog/ec2-packets-per-second-guaranteed-throughput-vs-best-effort" rel="noopener">blog</a> calls it Guaranteed vs. Best Effort. Basically, you’re allowed to burst for a while, but after a certain amount of time, you’ll get throttled. Finally, there is a per-flow limit of 10Gbs. So if you’re doing things like IPSEC, GRE, VXLAN, etc, note that you will never go any faster than 10g.</p><h3 id="closing-thoughts">Closing thoughts</h3><p>Throughout this blog, I mentioned the word ‘limits’ quite a bit, which has a bit of a negative connotation. However, it’s important to keep in mind that AWS is a multi-tenant environment, and it’s their job to make sure the user experience is still as much as possible as if the instance is dedicated to you. So you can also think of them as ‘guarantees’. AWS will not call them that, but in my experience, the throughput tests have been pretty reproducible with, say a +/- 10% measurement margin.</p><p>All in all, it’s pretty cool to be able to do 100G on AWS. As long as you are aware of the various limitations, which unfortunately aren’t well documented. Hopefully, this article helps some of you with that in the future. <br>Finally, could you use AWS to run your virtual firewalls, proxies, VPN gateways, etc? Sure, as long as you’re aware of the performance constraints. And with that design a horizontally scalable design, according to AWS best practices. The one thing you really do need to keep an eye on is the (egress) bandwidth pricing, which, when you started doing many gigabits per second, can add up.</p><p><em>Cheers</em><br><em>- Andree</em></p><p><br></p>]]></content:encoded></item><item><title><![CDATA[Building a global anycast service in under a minute]]></title><description><![CDATA[<p>This weekend I decided to take another look at Stackpath and their workload edge compute features. This is a relatively new feature, in fact, <a href="https://medium.com/@atoonk/experimenting-with-stackpath-edge-computing-and-anycast-f335ba86e60d?source=friends_link&amp;sk=ab2c1ae803a9f3ed314624f4545edde4" rel="noopener">I wrote about it in Feb 2109 when it was just released</a>. I remember being quite enthusiastic about the potential but also observed some things that</p>]]></description><link>http://toonk.io/building-a-global-anycast-service-in-under-a-minute/</link><guid isPermaLink="false">5f531ce73680d46e73ec5fdf</guid><category><![CDATA[terraform]]></category><category><![CDATA[anycast]]></category><dc:creator><![CDATA[Andree Toonk]]></dc:creator><pubDate>Sun, 21 Jun 2020 05:15:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1527685216219-c7bee79b0089?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1527685216219-c7bee79b0089?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Building a global anycast service in under a minute"><p>This weekend I decided to take another look at Stackpath and their workload edge compute features. This is a relatively new feature, in fact, <a href="https://medium.com/@atoonk/experimenting-with-stackpath-edge-computing-and-anycast-f335ba86e60d?source=friends_link&amp;sk=ab2c1ae803a9f3ed314624f4545edde4" rel="noopener">I wrote about it in Feb 2109 when it was just released</a>. I remember being quite enthusiastic about the potential but also observed some things that were lacking back then. Now, one and a half years later, it seems most of those have been resolved, so let’s take a look!</p><blockquote><em><em>I’ve decided to experiment with adding a small demo video to these blogs.</em></em><br><em><em>Below you will find a quick 5min demo of the whole setup. Since these videos are new and a bit of an experiment, let me know if you like it.</em></em></blockquote><figure class="kg-card kg-embed-card kg-card-hascaption"><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FoP3_wearfo0%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DoP3_wearfo0&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FoP3_wearfo0%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" allowfullscreen frameborder="0" height="480" width="640" title="Building a global anycast service in under a minute" class="s t u kq ai" scrolling="auto" style="box-sizing: inherit; position: absolute; top: 0px; left: 0px; width: 680px; height: 510px;"></iframe><figcaption>Demo: Building a global anycast service in under a minute</figcaption></figure><h1 id="workloads">Workloads</h1><p>Stackpath support two types of workloads (in addition to serverless), VM and container-based deployments. Both can be orchestrated using API’s and Terraform. Terraform is an “Infrastructure as code” tool. You simply specify your intent with Terraform, apply it, and you’re good to go. I’m a big fan of Terraform, so we’ll use that for our test.</p><p>One of the cool things about Stackpath is that they have built-in support for Anycast, for both their VM and Container service. I’m going to use that feature and the Container service to build this highly available, low latency web service. It’s super easy, see for your self <a href="https://github.com/atoonk/stackpath-anycast-terraform" rel="noopener nofollow">on my github here</a>.</p><h1 id="docker-setup">Docker setup</h1><p>Since I’m going to use the container service, we need to create a Docker container to work with. This is my <a href="https://github.com/atoonk/stackpath-anycast-terraform/blob/master/container/Dockerfile" rel="noopener nofollow">Dockerfile</a></p><!--kg-card-begin: markdown--><pre><code>FROM python:3
WORKDIR /usr/src/app
COPY ./mywebserver.py .
EXPOSE 8000
ENV PYTHONUNBUFFERED 1
CMD [ “python”, “./mywebserver.py” ]</code></pre>
<!--kg-card-end: markdown--><p>The <a href="https://github.com/atoonk/stackpath-anycast-terraform/blob/master/container/mywebserver.py" rel="noopener nofollow"><em><em>mywebserver.py</em></em></a> program is a simple web service that prints the hostname environment variable. This will help us determine which node is servicing our request when we start our testing.</p><p>After I built the container, I uploaded it to my <a href="https://hub.docker.com/repository/docker/atoonk/pythonweb" rel="noopener nofollow">Dockerhub</a> repo, so that Stackpath can pull it from there.</p><h1 id="terraform">Terraform</h1><p>Now it’s time to define our infrastructure using terraform. The relevant code can be found on my <a href="https://github.com/atoonk/stackpath-anycast-terraform/blob/master/container/main.tf" rel="noopener nofollow">github here</a>. I’ll highlight a few parts:</p><p>On line 17 we start with defining a new workload, and I’m requesting an Anycast IP for this workload. This means that Stackpath will load balance (ECMP) between all nodes in my workload (which I’m defining later).</p><!--kg-card-begin: markdown--><pre><code>resource “stackpath_compute_workload” “my-anycast-workload” {   
    name = “my-anycast-workload”
    slug = “my-anycast-workload”   
    annotations = {       
        # request an anycast IP       
        “anycast.platform.stackpath.net” = “true”   
    }
</code></pre>
<!--kg-card-end: markdown--><p>On line 31, we define the type of workload, in this case, a container. As part of that we’re opening the correct ports, in my case port 8000 for the python service.</p><!--kg-card-begin: markdown--><pre><code>container {   
    # Name that should be given to the container   
    name = “app”   
    port {      
        name = “web”      
        port = 8000      
        protocol = “TCP”      
        enable_implicit_network_policy = true   
    }
</code></pre>
<!--kg-card-end: markdown--><p>Next up we define the container we’d like to deploy (from Dockerhub)</p><!--kg-card-begin: markdown--><pre><code># image to use for the container
image = “atoonk/pythonweb:latest”
</code></pre>
<!--kg-card-end: markdown--><p>In the resources section we define the container specifications. In my case I’m going with a small spec, of one CPU core and 2G of ram.</p><!--kg-card-begin: markdown--><pre><code>resources {
   requests = {
      “cpu” = “1”
      “memory” = “2Gi”
   }
}
</code></pre>
<!--kg-card-end: markdown--><p>We now get to the section where we define how many containers we’d like per datacenter and in what datacenters we’d like this service to run.</p><p>In the example below, we’re deploying three containers in each datacenter, with the possibility to grow to four as part of auto-scaling. We’re deploying this in both Seattle and Dallas.</p><!--kg-card-begin: markdown--><pre><code>target {
    name         = &quot;global&quot;
    min_replicas = 3
    max_replicas = 4
    scale_settings {
      metrics {
        metric = &quot;cpu&quot;
        # Scale up when CPU averages 50%.
        average_utilization = 50
      }
    }
    # Deploy these instances to Dallas and Seattle
    deployment_scope = &quot;cityCode&quot;
    selector {
      key      = &quot;cityCode&quot;
      operator = &quot;in&quot;
      values   = [
        &quot;DFW&quot;, &quot;SEA&quot;
      ]
    }
  }
</code></pre>
<!--kg-card-end: markdown--><h1 id="time-to-bring-up-the-service-">Time to bring up the service.</h1><p>Now that we’ve defined our intent with terrraform, it’s time to bring this up. The proper way to do this is:</p><!--kg-card-begin: markdown--><pre><code>terraform init
terraform plan
terraform apply</code></pre>
<!--kg-card-end: markdown--><blockquote><em><em>After that, you’ll see the containers come up, and our anycasted python service will become available. Since the containers come up rather quickly, you should have all six containers in the two datacenters up and running in under a minute.</em></em></blockquote><h1 id="testing-the-load-balancing-">Testing the load balancing.</h1><p>I’ve deployed the service in both Seattle and Dallas, and since I am based in Vancouver Canada, I expect to hit the Seattle datacenter as that is the closest datacenter for me.</p><!--kg-card-begin: markdown--><pre><code class="language-bash">$ for i in `seq 1 10`; do curl 185.85.196.41:8000 ; done

my-anycast-workload-global-sea-2
my-anycast-workload-global-sea-0
my-anycast-workload-global-sea-2
my-anycast-workload-global-sea-0
my-anycast-workload-global-sea-1
my-anycast-workload-global-sea-1
my-anycast-workload-global-sea-2
my-anycast-workload-global-sea-1
my-anycast-workload-global-sea-2
my-anycast-workload-global-sea-0</code></pre>
<!--kg-card-end: markdown--><p>The results above show that I am indeed hitting the Seattle datacenter, and that my requests are being load balanced over the three instances in Seattle, all as expected.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://toonk.io/content/images/2020/09/image.png" class="kg-image" alt="Building a global anycast service in under a minute" srcset="http://toonk.io/content/images/size/w600/2020/09/image.png 600w, http://toonk.io/content/images/size/w1000/2020/09/image.png 1000w, http://toonk.io/content/images/2020/09/image.png 1400w" sizes="(min-width: 720px) 720px"><figcaption>In the portal, I can see the per container logs as well</figcaption></figure><h1 id="in-summary">In Summary</h1><p>Compared to <a href="https://medium.com/@atoonk/experimenting-with-stackpath-edge-computing-and-anycast-f335ba86e60d?source=friends_link&amp;sk=ab2c1ae803a9f3ed314624f4545edde4" rel="noopener">my test last year </a>with Stackpath, there has been a nice amount of progress. It’s great to now be able to do all of this with just a Terraform file. It’s kind of exciting you can bring up a fully anycast service in under a minute with only one command! By changing the replicate number in the Terraform file we can also easily grow and shrink our deployment if needed.<br>In this article we looked at the container service only, but the same is possible with Virtual machines, my github repo has an example for that as well.</p><p><em><em>Finally, don’t forget to check the </em></em><a href="https://youtu.be/oP3_wearfo0" rel="noopener nofollow"><em><em>demo recording </em></em></a><em><em>and let me know if you’d like to see more video content.</em></em></p>]]></content:encoded></item><item><title><![CDATA[Building an XDP (eXpress Data Path) based BGP peering router]]></title><description><![CDATA[<p>Over the last few years, we’ve seen an increase in projects and initiatives to speed up networking in Linux. Because the Linux kernel is slow when it comes to forwarding packets, folks have been looking at userland or kernel bypass networking. In the last few blog posts, we’ve</p>]]></description><link>http://toonk.io/building-an-xdp-express-data-path-based-bgp-peering-router/</link><guid isPermaLink="false">5f53ccd83680d46e73ec6039</guid><category><![CDATA[networking]]></category><dc:creator><![CDATA[Andree Toonk]]></dc:creator><pubDate>Sun, 19 Apr 2020 00:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1555496710-2660a86c4557?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1555496710-2660a86c4557?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Building an XDP (eXpress Data Path) based BGP peering router"><p>Over the last few years, we’ve seen an increase in projects and initiatives to speed up networking in Linux. Because the Linux kernel is slow when it comes to forwarding packets, folks have been looking at userland or kernel bypass networking. In the last few blog posts, we’ve looked at examples of this, mostly leveraging DPDK to speed up networking. The trend here is, let’s just take networking away from the kernel and process them in userland. Great for speed, not so great for all the Kernel network stack features that now have to be re-implemented in userland.</p><p>The Linux Kernel community has recently come up with an alternative to userland networking, called XDP, Express data path, it tries to strike a balance between the benefits of the kernel and faster packet processing. In this article, we’ll take a look at what it would take to build a Linux router using XDP. We will go over what XDP is, how to build an XDP packet forwarder combined with a BGP router, and of course, look at the performance.</p><h1 id="xdp-express-data-path-">XDP (eXpress Data Path)</h1><p>XDP (eXpress Data Path) is an eBPF based high-performance data path merged in the Linux kernel since version 4.8. Yes, BPF, the same Berkeley packet filter as you’re likely familiar with from tcpdump filters, though that’s now referred to as Classic BPF. Enhanced BPF has gained a lot of popularity over the last few years within the Linux community. BPF allows you to connect to Linux kernel hook points, each time the kernel reaches one of those hook points, it can execute an eBPF program. I’ve heard some people describe eBPF as what Java script was for the web, an easy way to enhance the ’web’, or in this case, the kernel. With BPF you can execute code without having to write kernel modules. XDP, as part of the BPF family, operates early on in the Kernel network code. The idea behind XDP is to add an early hook in the RX path of the kernel and let a user-supplied eBPF program decide the fate of the packet. The hook is placed in the NIC driver just after the interrupt processing and before any memory allocation needed by the network stack itself. So all this happens before an SKB (the most fundamental data structure in the Linux networking code) is allocated. Practically this means this is executed before things like tc and iptables.</p><p>A BPF program is a small virtual machine, perhaps not the typical virtual machines you’re familiar with, but a tiny (RISC register machine) isolated environment. Since it’s running in conjunction with the kernel, there are some protective measures that limit how much code can be executed and what it can do. For example, it can not contain loops (only bounded loops), there are a limited number of eBPF instructions and helper functions. The maximum instruction limit per program is restricted to 4096 BPF instructions, which, by design, means that any program will terminate quickly. For kernel newer than 5.1, this limit was lifted to 1 million BPF instructions.</p><h1 id="when-and-where-is-the-xdp-code-executed">When and Where is the XDP code executed</h1><p>XDP programs can be attached to three different points. The fastest is to have it run on the NIC itself, for that you need a smartnic and is called offload mode. To the best of my knowledge, this is currently only supported on Netronome cards. The next attachment opportunity is essentially in the driver before the kernel allocates an SKB. This is called “native” mode and means you need your driver to support this, luckily <a href="https://github.com/xdp-project/xdp-project/blob/master/areas/drivers/README.org" rel="noopener nofollow">most popular drivers do nowadays</a>.</p><p>Finally, there is SKB or Generic Mode XDP, where the XDP hook is called from <em><em>netif _ receive _ skb()</em></em>, this is after the packet DMA and skb allocation are completed, as a result, you lose most of the performance benefits.</p><p>Assuming you don’t have a smartnic, the best place to run your XDP program is in native mode as you’ll really benefit from the performance gain.</p><h1 id="xdp-actions">XDP actions</h1><p>Now that we know that XDP code is an eBPF C program, and we understand where it can run, now let’s take a look at what you can do with it. Once the program is called, it receives the packet context and from that point on you can read the content, update some counters, potentially modify the packet, and then the program needs to terminate with one of 5 XDP actions:</p><p><strong><strong>XDP_DROP</strong></strong><br>This does exactly what you think it does; it drops the packet and is often used for XDP based firewalls and DDOS mitigation scenarios.<br><strong><strong>XDP_ABORTED</strong></strong><br>Similar to DROP, but indicates something went wrong when processing. This action is not something a functional program should ever use as a return code.<br><strong><strong>XDP_PASS</strong></strong><br>This will release the packet and send it up to the kernel network stack for regular processing. This could be the original packet or a modified version of it.<br><strong><strong>XDP_TX</strong></strong><br>This action results in bouncing the received packet back out the same NIC it arrived on. This is usually combined with modifying the packet contents, like for example, rewriting the IP and Mac address, such as for a one-legged load balancer.<br><strong><strong>XDP_REDIRECT</strong></strong><br>The redirect action allows a BPF program to redirect the packet somewhere else, either a different CPU or different NIC. We’ll use this function later to build our router. It is also used to implement AF_XDP, a new socket family that solves the highspeed packet acquisition problem often faced by virtual network functions. AF_XDP is, for example, used by IDS’ and now also supported by Open vSwitch.</p><h1 id="building-an-xdp-based-high-performant-router">Building an XDP based high performant router</h1><p>Alright, now that we have a better idea of what XDP is and some of its capabilities, let’s start building! My goal is to build an XDP program that forwards packets at line-rate between two 10G NICs. I also want the program to use the regular Linux routing table. This means I can add static routes using the “ip route” command, but it also means I could use an opensource BGP daemon such as Bird or FRR.</p><p>We’ll jump straight to the code. I’m using the excellent XDP tutorial code to get started.<a href="https://github.com/atoonk/xdp-tutorial/blob/master/packet-solutions/xdp_prog_kern_03.c#L225-L320" rel="noopener nofollow"> I forked it here</a>, but it’s mostly the same code as the original. This is an example called “xdp_router” and uses the <em><em>bpf_fib_lookup()</em></em> function to determine the egress interface for a given packet using the Linux routing table. The program then uses the action <em><em>bpf_redirect_map()</em></em> to send it out to the correct egress interface. <a href="https://github.com/atoonk/xdp-tutorial/blob/master/packet-solutions/xdp_prog_kern_03.c#L225-L335" rel="noopener nofollow">You can see code here</a>. It’s only a hundred lines of code to do all the work.</p><p>After we compile the code (just run make in the parent directory), we load the code using the <em><em>./xdp_loader</em></em> program included in the repo and use the .<em><em>/xdp_prog_user</em></em> program to populate and query the redirect_params maps.</p><pre><code>#pin BPF resources (redirect map) to a persistent filesystem
mount -t bpf bpf /sys/fs/bpf/

# attach xdp_router code to eno2
./xdp_loader -d eno2 -F — progsec xdp_router

# attach xdp_router code to eno4
./xdp_loader -d eno4 -F — progsec xdp_router

# populate redirect_params maps
./xdp_prog_user -d eno2
./xdp_prog_user -d eno4</code></pre><h1 id="test-setup">Test setup</h1><p>So far, so good, we’ve built an XDP based packet forwarder! For each packet that comes in on either network interface eno2 or eno4 it does a route lookup and redirects it to the correct egres interface, all in eBPF code. All in a hundred lines of code, Pretty awesome, right?! Now let’s measure the performance to see if it’s worth it. Below is the test setup.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://toonk.io/content/images/2020/09/image-1.png" class="kg-image" alt="Building an XDP (eXpress Data Path) based BGP peering router" srcset="http://toonk.io/content/images/size/w600/2020/09/image-1.png 600w, http://toonk.io/content/images/size/w1000/2020/09/image-1.png 1000w, http://toonk.io/content/images/2020/09/image-1.png 1400w" sizes="(min-width: 720px) 720px"><figcaption>test setup</figcaption></figure><p>I’m using <a href="https://medium.com/@atoonk/building-a-high-performance-linux-based-traffic-generator-with-dpdk-93bb9904416c?source=friends_link&amp;sk=fb002771094f1f79654ec9a8be5761c7" rel="noopener">the same traffic generator as before </a>to generate 14Mpps at 64Bytes for each 10G link. Below are the results:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://toonk.io/content/images/2020/09/image-2.png" class="kg-image" alt="Building an XDP (eXpress Data Path) based BGP peering router" srcset="http://toonk.io/content/images/size/w600/2020/09/image-2.png 600w, http://toonk.io/content/images/size/w1000/2020/09/image-2.png 1000w, http://toonk.io/content/images/2020/09/image-2.png 1400w" sizes="(min-width: 720px) 720px"><figcaption>XDP forwarding Test results</figcaption></figure><p>The results are amazing! A single flow in one direction can go as high as 4.6 Mpps, using one core. Earlier, we saw the Linux kernel can go as high as 1.4Mpps for one flow using one core.</p><p>14Mpps in one direction between the two NICs require four cores. Our earlier blog showed that the regular kernel would need 16 cores to do this work!</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="http://toonk.io/content/images/2020/09/image-3.png" class="kg-image" alt="Building an XDP (eXpress Data Path) based BGP peering router" srcset="http://toonk.io/content/images/size/w600/2020/09/image-3.png 600w, http://toonk.io/content/images/size/w1000/2020/09/image-3.png 1000w, http://toonk.io/content/images/2020/09/image-3.png 1600w" sizes="(min-width: 1200px) 1200px"><figcaption>Test result — XDP forwarding using XDP_REDIRECT, 5 cores to forward 29Mpps</figcaption></figure><p>Finally, for the bidirectional 10,000 flow test, forwarding 28Mpps, we need five cores. All tests are significantly faster than forwarding packets using the regular kernel and all that with minor changes to the system.</p><h2 id="just-so-you-know">Just so you know</h2><p>Since all packet forwarding happens in XDP, packets redirected by XDP won’t be visible to IPtables or even tcpdump. Everything happens before packets even reach that layer, and since we’re redirecting the packet, it never moves up higher the stack. So if you need features like ACLs or NAT, you will have to implement that in XDP (take a look at <a href="https://cilium.io/" rel="noopener nofollow">https://cilium.io/</a>).</p><p></p><!--kg-card-begin: markdown--><p><strong>A word on measuring cpu usage.</strong><br>
To control and measure the number of CPU cores used by XDP, I’m changing the number of queues the NIC can use. I increase the number of queues on my XL710 Intel NIC incrementally until I get a packet loss-free transfer between the two ports on the traffic generator. For example, to get 14Mpps in one direction from port 0 to 1 on the traffic generator through our XDP router, which was forwarding between eno2 and eno4, I used the following settings:</p>
<pre><code>ethtool -L eno2 combined 4
ethtool -L eno4 combined 4</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>For the 28Mpps testing, I used the following</p>
<pre><code>ethtool -L eno2 combined 4
ethtool -L eno4 combined 4</code></pre>
<!--kg-card-end: markdown--><blockquote><em><em><strong>A word of caution</strong></em></em><br><em><em>Interestingly, increasing the number of queues, and thus using more cores, appears to, in some cases, have a negative impact on the efficiency. Ie. I’ve seen scenarios when using 30 queues, where the unidirectional 14mps test with 10,000 flows appear to use almost no CPU (between 1 and 2) while the same test bidirectionally uses up all 30 cores. When restarting this test, I see some inconsistent behavior in terms of CPU usage, so not sure what’s going on, I will need to spend a bit more time on this later.</em></em></blockquote><h1 id="xdp-as-a-peering-router"><strong>XDP as a peering router</strong></h1><p>The tests above show promising results, but one major difference between a simple forwarding test and a real life peering router is the number of routes in the forwarding table. So the questions we need to answer was how the <em><em>bpf_fib_lookup</em></em> function will perform when there are more than just a few routes in the routing table. More concretely, <strong><strong><em><em>could you use Linux with XDP as a full route peering router?</em></em></strong></strong><br>To answer this question, I installed <a href="https://bird.network.cz/" rel="noopener nofollow">bird</a> as a bgp daemon on the XDP router. Bird has a peering session with an <a href="https://github.com/Exa-Networks/exabgp" rel="noopener nofollow">exabgp</a> instance, which I loaded with a full routing table using <a href="https://github.com/t2mune/mrtparse/blob/master/examples/mrt2exabgp.py" rel="noopener nofollow">mrt2exabgp.py</a> and a MRT files from <a href="https://www.ripe.net/analyse/internet-measurements/routing-information-service-ris/ris-raw-data" rel="noopener nofollow">RIPE RIS</a>.<br>Just to be a <em><em>real</em></em> peering router, I also filtered out the RPKI invalid routes using <a href="https://github.com/job/rtrsub" rel="noopener nofollow">rtrsub</a>. The end result is a full routing table with about 800k routes in the Linux FIB.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="http://toonk.io/content/images/2020/09/image-4.png" class="kg-image" alt="Building an XDP (eXpress Data Path) based BGP peering router" srcset="http://toonk.io/content/images/size/w600/2020/09/image-4.png 600w, http://toonk.io/content/images/size/w1000/2020/09/image-4.png 1000w, http://toonk.io/content/images/size/w1600/2020/09/image-4.png 1600w, http://toonk.io/content/images/2020/09/image-4.png 2000w" sizes="(min-width: 1200px) 1200px"><figcaption>Test result — XDP router with a ful routing table. 5 cores to forward 28Mpps</figcaption></figure><blockquote>After re-running the performance tests with 800k bgp routes in the FIB, I observed <strong><em>no noticeable decrease in performance</em></strong>.</blockquote><blockquote>This indicates that a larger FIB table has no measurable impact on the XDP helper <em>bpf_fib_lookup</em>(). This is exciting news for those interested in a cheap and fast peering router.</blockquote><h2 id="conclusion-and-closing-thoughts-">Conclusion and closing thoughts.</h2><p>We started the article with a quick introduction to eBPF and XDP. We learned that XDP is a subset of the recent eBPF developments focused specifically on the hooks in the network stack. We went over the different XDP actions and introduced the redirect action, which, together with the bpf_fib_lookup helper allows us to build the XDP router.</p><p>When looking at the performance, we see this we can speed up <a href="https://medium.com/@atoonk/linux-kernel-and-measuring-network-throughput-547c3b68c4d2?source=friends_link&amp;sk=b8a74c58f8b7a0d998a8796410b3fb96">packet forwarding in Linux</a> by roughly five times in terms of CPU efficiency compared to regular kernel forwarding. We observed we needed about five cores to forward 28Mpps bidirectional between two 10G NICs.</p><p>When we compare these results with the results from my last blog,<a href="https://medium.com/swlh/kernel-bypass-networking-with-fd-io-and-vpp-fc3a53a669f9?source=friends_link&amp;sk=ab92fa42f7ffdfb6dca39ae9601f3d3e"> DPDK and VPP</a>, we see that XDP is slightly slower, ie. 3 cores (vpp) vs 5 cores (XDP) for the 28Mpps test. However, the nice part about working with XDP was that I was able to leverage the Linux routing table out of the box, which is a major advantage.</p><p>The exciting part is that this setup integrates natively with Netlink, which allowed us to use Bird, or really any other routing daemon, to populate the FIB. We also saw that the impact of 800K routes in the fib had no measurable impact on the performance.</p><blockquote>The fib_lookup helper function allowed us to build a router and leverage well-known userland routing daemons. I would love to also see a similar helper function for conntrack, or perhaps some other integration with Netfilter. It would make building firewalls and perhaps even NAT a lot easier. Punting the first packet to the kernel, and subsequent packets are handled by XDP.</blockquote><blockquote>Wrapping up, we started with the question can we build a high performant peering router using XDP? The answer is yes! You can build a high performant peering router using just Linux and relying on XDP to accelerate the dataplane. While leveraging the various open-source routing daemons to run your routing protocols. That’s exciting!</blockquote><blockquote>Cheers<br> -Andree</blockquote>]]></content:encoded></item><item><title><![CDATA[Kernel bypass networking with FD.io and VPP.]]></title><description><![CDATA[In this blog, we’ll look at VPP, a fast, flexible, and open-source network forwarding plane. I think you will be blown away by the performance numbers.]]></description><link>http://toonk.io/kernel-bypass-networking-with-fd-io-and-vpp/</link><guid isPermaLink="false">5f53d4c53680d46e73ec6097</guid><category><![CDATA[networking]]></category><dc:creator><![CDATA[Andree Toonk]]></dc:creator><pubDate>Sun, 05 Apr 2020 18:14:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1514900417871-7912cae3d726?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1514900417871-7912cae3d726?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Kernel bypass networking with FD.io and VPP."><p></p><p>Over the last few years, I have experimented with various flavors of userland, kernel-bypass networking. In this article, we’ll take <a href="https://fd.io/" rel="noopener">FD.IO</a> for a spin.</p><p>We will compare the result with the results of my <a href="https://medium.com/@atoonk/linux-kernel-and-measuring-network-throughput-547c3b68c4d2?source=friends_link&amp;sk=b8a74c58f8b7a0d998a8796410b3fb96">last blog</a> in which we looked at how much a vanilla Linux kernel could do in terms of forwarding (routing) packets. We observed that on Linux, to achieve 14Mpps we needed roughly 16 and 26 cores for a unidirectional and bidirectional test. In this article, we’ll look at what we need to accomplish this with FD.io</p><h3 id="userland-networking">Userland networking</h3><p>The principle of Userland networking is that the networking stack is no longer handled by the kernel, but instead by a userland program. The Linux kernel is incredibly feature-rich, but for fast networking, it also requires a lot of cores to deal with all the (soft) interrupts. Several of the userland networking projects rely on DPDK to achieve incredible numbers. One reason why DPDK is so fast is that it doesn’t rely on Interrupts. Instead, it’s a poll mode driver. Meaning it’s continuously spinning at 100% picking up packets from the NIC. A typical server nowadays comes with quite a few CPU cores, and dedicating one or more cores for picking packets of the NIC is, in some cases, entirely worth it. Especially if the server needs to process lots of network traffic.</p><p>So DPDK provides us with the ability to efficiently and extremely fast, send and receive packets. But that’s also it! Since you’re not using the kernel, we now need a program that takes the packets from DPDK and does something with it. Like for example, a virtual switch or router.</p><h3 id="fd-io">FD.IO</h3><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1200/1*xnuX9v2emwe179_6PGIp7g.png" class="kg-image" alt="Kernel bypass networking with FD.io and VPP."></figure><p><a href="https://fd.io/" rel="noopener">FD.IO</a> is an open-source software dataplane developed by Cisco. At the heart of FD.io is something called Vector Packet Processing (VPP).</p><p>The VPP platform is an <a href="https://fd.io/vppproject/vpptech/" rel="noopener">extensible framework </a>that provides switching and routing functionality. VPP is built on a ‘packet processing graph.’ This modular approach means that anyone can ‘plugin’ new graph nodes. This makes extensibility rather simple, and it means that plugins can be customized for specific purposes.</p><p>FD.io can use DPDK as the drivers for the NIC and can then process the packets at a high performant rate that can run on commodity CPU. It’s important to remember that it is not a fully-featured router, ie. it doesn’t really have a control plane; instead, it’s a forwarding engine. Think of it as a router line-card, with the NIC and the DPDK drivers as the ports. VPP allows us to take a packet from one NIC to another, transform it if needed, do table lookups, and send it out again. There are API’s that allow you to manipulate the forwarding tables. Or you can use the CLI to, for example, configure static routes, VLAN, vrf’s etc.</p><h3 id="test-setup">Test setup</h3><p>I’ll use mostly the same test setup as in my previous test. Again using two<a href="https://www.packet.com/cloud/servers/n2-xlarge/" rel="noopener"> n2.xlarge.x86</a> servers from<a href="https://www.packet.com/" rel="noopener"> packet.com</a> and our <a href="https://medium.com/@atoonk/building-a-high-performance-linux-based-traffic-generator-with-dpdk-93bb9904416c?source=friends_link&amp;sk=fb002771094f1f79654ec9a8be5761c7">DPDK traffic generator</a>. The set up is as below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1600/1*bPwcK3XnekCTdwU3HVgeeQ.png" class="kg-image" alt="Kernel bypass networking with FD.io and VPP."><figcaption>Test setup</figcaption></figure><p>I’m using the VPP code from the FD.io master branch and installed it on a vanilla Ubuntu 18.04 system <a href="https://fd.io/docs/vpp/master/gettingstarted/installing/ubuntu.html" rel="noopener">following these steps</a>.</p><h3 id="test-results-packet-forwarding-using-vpp">Test results — Packet forwarding using VPP</h3><p>Now that we have our test setup ready to go, it’s time to start our testing!<br>To start, I configured VPP with “vppctl” like this, note that I need to set static ARP entries since the packet generator doesn’t respond to ARP.</p><p>set int ip address TenGigabitEthernet19/0/1 10.10.10.2/24<br>set int ip address TenGigabitEthernet19/0/3 10.10.11.2/24<br>set int state TenGigabitEthernet19/0/1 up<br>set int state TenGigabitEthernet19/0/3 up<br>set ip neighbor TenGigabitEthernet19/0/1 10.10.10.3 e4:43:4b:2e:b1:d1<br>set ip neighbor TenGigabitEthernet19/0/3 10.10.11.3 e4:43:4b:2e:b1:d3</p><p>That’s it! Pretty simple right?</p><p>Ok, time to look at the results just like before we did a single flow test, both unidirectional and bidirectional, as well as a 10,000 flow test.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1600/1*SOuQiKUviuPLjlOlAlAP8Q.png" class="kg-image" alt="Kernel bypass networking with FD.io and VPP."><figcaption>VPP forwarding test&nbsp;results</figcaption></figure><p>Those are some remarkable numbers! With a single flow, VPP can process and forward about 8Mpps, not bad. The perhaps more realistic test with 10,000 flows, shows us that it can handle 14Mpps with just two cores. To get to a full bi-directional scenario where both NICs are sending and receiving at line rate (28 Mpps per NIC) we need three cores and three receive queues on the NIC. To achieve this last scenario with Linux, we needed approximately 26 cores. Not bad, not bad at all!</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/2400/1*CnHwc00tX2BSnE5c424zuA.png" class="kg-image" alt="Kernel bypass networking with FD.io and VPP."><figcaption>Traffic generator on the left, VPP server on the right. This shows the full line-rate bidirectional test: 14Mpps per NIC, while VPP uses 3&nbsp;cores.</figcaption></figure><h3 id="test-results-nat-using-vpp">Test results — NAT using VPP</h3><p>In my previous blog we saw that when doing SNAT on Linux with iptables, we got as high as 3Mpps per direction needing about 29 CPUs per direction. This showed us that packet rewriting is significantly more expensive than just forwarding. Let’s take a look at how VPP does nat.</p><p>To enable nat on VPP, I used the following commands:</p><pre><code>nat44 add interface address TenGigabitEthernet19/0/3
nat addr-port-assignment-alg default
set interface nat44 in TenGigabitEthernet19/0/1 out TenGigabitEthernet19/0/3 output-feature</code></pre><p>My first test is with one flow only in one direction. With that, I’m able to get 4.3Mpps. That’s’ exactly half of what we saw in the performance test without nat. It’s no surprise this is slower due to the additional work needed. Note that with Linux iptables I was seeing about 1.1Mpps.</p><p>A single flow for nat isn’t super representative of a real-life nat example where you’d be translating many sources. So for the next measurements, I’m using 255 different source IP addresses and 255 destination IP addresses as well as different port numbers; with this setup, the nat code is seeing about 16k sessions. I can now see the numbers go to 3.2Mpps; more flows mean more nat work. Interestingly, this number is exactly the same as I saw with iptables. There is however one big difference, with iptables the system was using about 29 cores. In this test, I’m only using two cores. That’s a low number of workers, and also the reason I’m capped. To remove that cap, I added more cores and validated that the VPP code scales horizontally. Eventually, I need 12 cores to run 14Mpps for a stable experience.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1600/1*Rrh8rnpAB-SMMqdpe5pG5w.png" class="kg-image" alt="Kernel bypass networking with FD.io and VPP."><figcaption>VPP forwarding with NAT test&nbsp;results</figcaption></figure><p>Below is the relevant VPP config to control the number of cores used by VPP. Also, I should note that I <a href="https://www.linuxtopia.org/online_books/linux_kernel/kernel_configuration/re46.html" rel="noopener">isolated the cores</a> I allocated to VPP so that the kernel wouldn’t schedule anything else on it.</p><pre><code>cpu {
    main-core 1
    # CPU placement:
    corelist-workers 3–14
    # Also added this to grub: isolcpus=3-31,34-63
}
dpdk {
   dev default {
      # RSS, number of queues
      num-rx-queues 12
      num-tx-queues 12
      num-rx-desc 2048
      num-tx-desc 2048
   }
   dev 0000:19:00.1
   dev 0000:19:00.3
}
plugins {
   plugin default { enable }
   plugin dpdk_plugin.so { enable }
}
nat {
   endpoint-dependent
   translation hash buckets 1048576
   translation hash memory 268435456
   user hash buckets 1024
   max translations per user 10000
 }</code></pre><h3 id="conclusion">Conclusion</h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1200/0*6AoxbyuX5YbpSjeu" class="kg-image" alt="Kernel bypass networking with FD.io and VPP."><figcaption>Photo by <a href="https://unsplash.com/@nci?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/@nci?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-creator noopener noopener" target="_blank">National Cancer Institute</a> on&nbsp;<a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-source noopener noopener" target="_blank">Unsplash</a></figcaption></figure><p>In this blog, we looked at VPP from the FD.io project as a userland forwarding engine. VPP is one example of a kernel bypass method for processing packets. It works closely with and further augments DPDK.</p><p>We’ve seen that the VPP code is <a href="https://wiki.fd.io/view/VPP/What_is_VPP%3F#Feature_Rich" rel="noopener">feature-rich</a>, especially for a kernel bypass packet forwarder. Most of all, <strong>it’s crazy fast</strong>.</p><p>We need just three cores to have two NICs forward full line-rate (14Mpps) in both directions. Comparing that to the Linux kernel, which needed 26 cores, we see an almost 9x increase in performance. <br>We noticed that the results were even better when using nat. In Linux, I wasn’t able to get any higher than 3.2Mpps for which I needed about 29 cores. With VPP we can do 3.2Mpps with just two cores and get to full line rate nat with 12 cores.</p><p>I think <a href="https://fd.io/" rel="noopener">FD.io</a> is an interesting and exciting project, and I’m a bit surprised it’s not more widely used. One of the reasons is likely that there’s a bit of a learning curve. But if you need high-performance packet forwarding, it’s certainly something to explore! Perhaps this is the start of your VPP project? if so, <a href="https://twitter.com/atoonk" rel="noopener">let me know!</a></p><p><em>Cheers</em><br><em> -Andree</em></p>]]></content:encoded></item></channel></rss>